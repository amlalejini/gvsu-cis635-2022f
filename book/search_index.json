[["index.html", "GVSU CIS 635 - Knowledge Discovery and Data Mining (Fall 2022) Chapter 1 Introduction", " GVSU CIS 635 - Knowledge Discovery and Data Mining (Fall 2022) Instructor: Dr. Alexander Lalejini 2022-10-17 Chapter 1 Introduction This eBook contains some of the course materials for the Fall 2022 sections 02 and 03 of CIS 635 at Grand Valley State University. "],["getting-started-with-r-markdown.html", "Chapter 2 Getting Started with R Markdown 2.1 Dependencies 2.2 Create a new R Markdown document in RStudio 2.3 Anatomy of an R Markdown document 2.4 “Knitting” 2.5 Resources", " Chapter 2 Getting Started with R Markdown R markdown documents are a way to interweave markdown with R. This enables us to write nicely formatted documents with R code (and output from your R code) embedded directly in the document. Find examples of compiled R markdown documents here: https://rmarkdown.rstudio.com/gallery.html In fact, this page is generated with R Markdown! Source code for this page here. 2.1 Dependencies To use R markdown, you’ll need to install the following packages: markdown and rmarkdown. You should also want to go ahead and install the tinytex package (+ install TinyTex) to be able generate PDF output from your R Markdown documents. See the instructions below! 2.1.1 Installing R packages As previously mentioned, there are many R packages (&gt; 18,000!) with all sorts of useful functionality. To install an R package, use the install.packages function in your R console (e.g., in RStudio). Run ?install.packages in your R console to read its help page. To install the markdown and rmarkdown packages, run the following command in your R console: install.packages(c(&quot;markdown&quot;, &quot;rmarkdown&quot;)) Once the installations finish, you should be able to run the following two commands without errors: library(markdown) library(rmarkdown) For this class, you’ll also need to be able to generate PDF output from your R Markdown documents. To do so, you’ll need to instal LaTex (if you’re not sure what that is, no worries!). Run the following two commands in your R console in RStudio: install.packages(&#39;tinytex&#39;) tinytex::install_tinytex() 2.2 Create a new R Markdown document in RStudio Click the new file button in the top right of the RStudio window, and select R Markdown. In RStudio, you should get a pop-up for creating a new r markdown document. For now, there are two relevant options: Click the “Create Empty Document” in the bottom left of the popup. This will create an empty R markdown file for you. Fill out the title, author (you!), and date fields and press “Okay”. This will create a new R markdown file, but RStudio will initialize your file with a bunch of stuff (this stuff is helpful information, but you will want to delete it all for any assignments in this class). 2.3 Anatomy of an R Markdown document There are three basic components in an R Markdown document: the metadata, text, and code. Here’s the contents for a minimal R Markdown document (from https://bookdown.org/yihui/rmarkdown/basics.html): An R Markdown document should be a plain text file (i.e., don’t try to write one in google docs or microsoft word!) and conventionally has the .Rmd file extension. 2.3.1 Frontmatter (metadata) The metadata (or the frontmatter) specifies how your R Markdown should be compiled. E.g., the output file type, whether to include a table of contents, etc. The metadata should be written between a pair of three dashes --- at the top of your document (see minimal example). For now, don’t worry too much about what you should include in your document metadata. For the most part, you can just use what RStudio includes in your document. 2.3.2 Text The body of your document follows the metadata. Any text (i.e., everything that isn’t code) you include in your document should follow markdown syntax. If you’re not already familiar with markdown, read over this: https://www.markdownguide.org/getting-started. For basic markdown syntax, see: https://www.markdownguide.org/basic-syntax/. Being familiar with basic syntax should be just about everything you need for your homework assignments in this class. For more advanced markdown syntax, see: https://www.markdownguide.org/extended-syntax/. You can also refer to Section 2.5 in R Markdown: The Definitive Guide for more info on markdown. 2.3.3 Code In general, you can include code in your R Markdown documents in two ways: Inline code begin with `r and end with a `. E.g., 2 would render the result of log(4, base=2) inline. You can include a “code chunk” inside your R Markdown document. A code chunk begins with three backticks ```{r} and end with three backticks (see minimal example above for an example). There are many options tha tyou can specify in the {} at the beginning of a code chunk. See https://yihui.org/knitr/options/ for details. 2.4 “Knitting” To compile an R Markdown document into a PDF or HTML file, you need to “Knit” it. In RStudio, this is pretty straightforward. Just click the “Knit” button on your document in RStudio. 2.5 Resources R Markdown: The Definitive Guide Code chunk options: https://yihui.org/knitr/options/ "],["homework-1.html", "Chapter 3 Homework 1 3.1 Setup 3.2 Part A. Use R as a calculator 3.3 Part B. Built-in Functions 3.4 Part C. Creating vectors 3.5 Part D. Subsetting vectors 3.6 Part E. Types of data", " Chapter 3 Homework 1 This goal of this homework is as follows: Give you an opportunity to practice some R programming basics Make sure you are comfortable creating and knitting an R Markdown document Grading Each question is weighted evenly 3.1 Setup Make sure you have access to a computer with R and RStudio installed (with the ability to install new packages). If you do not, let me know as soon as possible! If you have not done so already, install R and RStudio (step-by-step guides provided in week 1 course content). I strongly encourage you to stay organized. I recommend that you directory on your computer where you can save all of your work for this class (e.g., “cis635” would make for a good name). Within that directory, I recommend keeping each of your homework assignments and projects separated into their own directories. For example, I might organize things as follows: cis635/ homeworks/ hw01 hw02 ... project/ ... Create a new R Markdown file with the title “Homework 1” and with you as the author (hint: this information will go into your R Markdown frontmatter at the top of the file). In your R markdown file, create a section heading for each of the following parts of your homework: Part A. Use R as a calculator Part B. Built-in Functions Part C. Creating vectors Part D. Subsetting vectors Part E. Types of data 3.2 Part A. Use R as a calculator Under your Part A heading, write one code chunk for each of the following calculations: \\[ 1+2(3+4) \\] \\[ log_2(4^3 + 3^{2+1}) \\] \\[ \\sqrt{(4+3)(2+1)} \\] For example, if the calculation is 2 + 2, your code chunk should look something like this when your R Markdown document is compiled: 2+2 ## [1] 4 I want to see each equation translated directly into code. Do not simplify any of the calculations (e.g., 2+1 should be 2+1 in your code, not 3). 3.3 Part B. Built-in Functions A built-in function is one that comes pre-loaded in R (you don’t need to install and load a package to use). To learn how to use a built-in loaded function that you don’t know how to use appropriately, use the help() function. help() takes one parameter, the name of the function that you want information about (e.g., help(abs)). Instead of the help function, you could also use enter ? and then the name of the function in your R console (e.g., ?abs). Familiarize yourself with the built-in functions abs(), round(), sqrt(), tolower(), and toupper(). Under your Part B heading, use these built-in functions to write code that prints the following items (put each of these into a different code chunk in your R Markdown document): The absolute value of -15.5. 4.483847 rounded to one decimal place. The function round() takes two arguments, the number to be rounded and the number of decimal places. “species” in all capital letters. “SPECIES” in all lower case letters. 3.4 Part C. Creating vectors Under your Part C heading, Create the following vectors using just seq() and/or rep(). (don’t use c()) Positive integers from 1 to 99 Odd integers between 1 and 99 The numbers 1,1,1,2,2,2,3,3,3 (hint: read the help pages for seq and rep!) 3.5 Part D. Subsetting vectors Under your Part D heading, use subsetting syntax (square brackets) to write code that completes the following using the vector y. y &lt;- c(3,2,15,-1,22,1,9,17,5) Display only the first value of y. Display the last value of y, in a way that would work if y were any length. (hint: ?length) Display only the values in y that are greater than the mean of y. 3.6 Part E. Types of data Under your part E heading, using the vector y write code that completes the following tasks: y &lt;- c(3,2,15,-1,22,1,9,17,5) Make a logical (TRUE/FALSE) vector describing which values in y are positive. Make a logical vector describing whether any of the values of y are equal to the mean of y. Coerce the vector you just made (in #2 above) from a logical vector to a character vector. Make a logical vector describing whether any of the values of y are equal to the median of y. Coerce the vector you just made (in #4 above) into a categorial vector (using factor()). Make a matrix with 4 rows and 3 columns that looks like this: ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## [4,] 10 11 12 Coerce that matrix into a dataframe. "],["week-02---overview.html", "Chapter 4 Week 02 - Overview 4.1 Dependencies", " Chapter 4 Week 02 - Overview This week, we’ll be practicing more R programming: Writing functions Reading/writing data tidyverse packages Using dplyr functionality to manipulate data Using tidyr functionality to reformat data Using ggplot2 to create graphics 4.1 Dependencies For this week, you’ll need to install the following R packages: install.packages(&quot;tidyverse&quot;) "],["functions-in-r.html", "Chapter 5 Functions in R 5.1 Argument order and default values 5.2 Writing your own functions 5.3 Return values 5.4 Default argument values 5.5 Resources", " Chapter 5 Functions in R You can download the R Markdown file used to generate this document (or webpage) here. I would encourage you to download that Rmd file, open it own your own computer in RStudio, and run the code chunks on your own (and modify things as you wish!). In this tutorial, We’ll go over argument order and default values when calling functions in R We’ll go over the basics of writing functions in R 5.1 Argument order and default values As you encountered last week, R has many built-in functions (e.g., abs, round, etc), and all of R packages that you install provide even more functions that you can use. Using an existing function is simple. Write the name of the function you want to use followed by the requisite arguments in parentheses. For example, value &lt;- -15 abs(value) ## [1] 15 or, log(4, base=2) ## [1] 2 If you look at the documentation for abs (enter ?abs into your R console), there’s only one argument (x), and x is required (i.e., notice that it has no default values). In contrast, run ?log function in your R console. Notice that there are two possible arguments for log, x and base. x is required, as it has no default value, but base is optional, defaulting to exp(1) if not provided. Let’s say we want to take the \\(log_3\\) of 27. To do so, we’ll need to specify both the x and base arguments, but there are a few ways that we can do that. For example, we could call log with each argument in the correct position: log(27, 3) ## [1] 3 We could call log as I did above with log(4, base=2): log(27, base=3) ## [1] 3 In this case, R relies on argument position to deduce which argument is which up until the first named variable. We could also specify all arguments using their names: log(x=27, base=3) ## [1] 3 If you specify arguments this way (using their names), argument order doesn’t matter. log(base=3, x=27) ## [1] 3 When calling a function with multiple arguments, I strongly encourage you to specify arguments by name. 5.2 Writing your own functions To write a function in R, the general structure is as follows: my_function &lt;- function(parameters) { # Perform some actions using the given parameters. # Optionally, return some value } We can write a function that rolls a die for us like so: # This function rolls a six-sided die. # Run ?sample in your console if you&#39;re curious about how the sample function # works. roll &lt;- function() { result &lt;- sample(x = 1:6, size = 1, replace = TRUE) return(result) } Currently, roll_d6 will always simulate rolling a six-sided die. We could modify our function definition to add an argument that specifies the size of the die: # This function simulates rolling a die with a given number of sides. roll &lt;- function(sides) { result &lt;- sample(x = 1:sides, size = 1, replace = TRUE) return(result) } Note that when we run the code chunk above, we are redefining our roll function. Challenge question: Try writing your own roll function that rolls an argument-specified number of dice (where the number of sides of each die is specified by an argument) and then returns the sum of those rolls. 5.3 Return values In the roll functions that we wrote, we explicitly specified what the functions should return. However, if you do not use return() to specify what should be returned, the last evaluated expression is returned automatically. If the last evaluated expression doesn’t produce a value, your function will not return a value. I generally prefer to use explicit return statements because it lets me quickly identify what my functions are returning. Here’s a silly function: silly_function &lt;- function(a) { if (a == 1) { a + 100 } else if (a == 2) { 200 } else if (a == 3) { return(300) } else { } } Try calling silly_function with different inputs to see what it returns. 5.4 Default argument values Just like many built-in functions have some arguments with defualt values, you can define your functions such their arguments have default values. All you need to do is specify the default value of an argument when you define the function. For example, if we wrote a function that calculates the price something with a sales tax included, we might specify a default tax rate for convenience. price_with_tax &lt;- function(price, tax_rate=0.06) { return(price+(price*tax_rate)) } 5.5 Resources Hands-on Programming with R (2.3, 2.4) Software carpentry lesson on writing functions in R "],["reading-and-writing-data.html", "Chapter 6 Reading and Writing Data 6.1 Writing data 6.2 Loading data", " Chapter 6 Reading and Writing Data You can download the R Markdown file used to generate this document (or webpage) here. I would encourage you to download that Rmd file, open it own your own computer in RStudio, and run the code chunks on your own (and modify things as you wish!). In this tutorial: I’ll walk you through how to write data stored in a dataframe as a .csv file. And, I’ll show you how to load data saved in a .csv file into a dataframe. 6.1 Writing data R comes preloaded with many datasets (for a complete list run library(help=\"datasets\")). For example, Orange is a small data set that gives growth data for 5 orange trees. head(Orange) ## Tree age circumference ## 1 1 118 30 ## 2 1 484 58 ## 3 1 664 87 ## 4 1 1004 115 ## 5 1 1231 120 ## 6 1 1372 142 If we wanted to write this dataset out to a csv file, we could use the write.csv function. write.csv(x=Orange, file=&quot;orange_trees.csv&quot;, row.names=FALSE) If you run the above R code, where did orange_trees.csv get saved? (hint: in your current working directory) write.csv has many options. Run ?write.csv in your R console to read the documentation. write.csv will be very useful when you want to clean up or transform a dataset that you’ll be analyzing/working with. If your dataset is large and/or the preprocessing operations you perform are expensive, you don’t want do those operations every time you load your dataset. Instead, write a script to process the dataset and save the processed dataset. 6.2 Loading data Loading data from a csv file is straightfoward. You’ll use the read.csv function. pokemon_df &lt;- read.csv(file=&quot;lecture-material/week-02/data/pokemon.csv&quot;) head(pokemon_df) ## abilities against_bug against_dark against_dragon ## 1 [&#39;Overgrow&#39;, &#39;Chlorophyll&#39;] 1.00 1 1 ## 2 [&#39;Overgrow&#39;, &#39;Chlorophyll&#39;] 1.00 1 1 ## 3 [&#39;Overgrow&#39;, &#39;Chlorophyll&#39;] 1.00 1 1 ## 4 [&#39;Blaze&#39;, &#39;Solar Power&#39;] 0.50 1 1 ## 5 [&#39;Blaze&#39;, &#39;Solar Power&#39;] 0.50 1 1 ## 6 [&#39;Blaze&#39;, &#39;Solar Power&#39;] 0.25 1 1 ## against_electric against_fairy against_fight against_fire against_flying ## 1 0.5 0.5 0.5 2.0 2 ## 2 0.5 0.5 0.5 2.0 2 ## 3 0.5 0.5 0.5 2.0 2 ## 4 1.0 0.5 1.0 0.5 1 ## 5 1.0 0.5 1.0 0.5 1 ## 6 2.0 0.5 0.5 0.5 1 ## against_ghost against_grass against_ground against_ice against_normal ## 1 1 0.25 1 2.0 1 ## 2 1 0.25 1 2.0 1 ## 3 1 0.25 1 2.0 1 ## 4 1 0.50 2 0.5 1 ## 5 1 0.50 2 0.5 1 ## 6 1 0.25 0 1.0 1 ## against_poison against_psychic against_rock against_steel against_water ## 1 1 2 1 1.0 0.5 ## 2 1 2 1 1.0 0.5 ## 3 1 2 1 1.0 0.5 ## 4 1 1 2 0.5 2.0 ## 5 1 1 2 0.5 2.0 ## 6 1 1 4 0.5 2.0 ## attack base_egg_steps base_happiness base_total capture_rate classfication ## 1 49 5120 70 318 45 Seed Pokémon ## 2 62 5120 70 405 45 Seed Pokémon ## 3 100 5120 70 625 45 Seed Pokémon ## 4 52 5120 70 309 45 Lizard Pokémon ## 5 64 5120 70 405 45 Flame Pokémon ## 6 104 5120 70 634 45 Flame Pokémon ## defense experience_growth height_m hp japanese_name name ## 1 49 1059860 0.7 45 Fushigidaneフシギダネ Bulbasaur ## 2 63 1059860 1.0 60 Fushigisouフシギソウ Ivysaur ## 3 123 1059860 2.0 80 Fushigibanaフシギバナ Venusaur ## 4 43 1059860 0.6 39 Hitokageヒトカゲ Charmander ## 5 58 1059860 1.1 58 Lizardoリザード Charmeleon ## 6 78 1059860 1.7 78 Lizardonリザードン Charizard ## percentage_male pokedex_number sp_attack sp_defense speed type1 type2 ## 1 88.1 1 65 65 45 grass poison ## 2 88.1 2 80 80 60 grass poison ## 3 88.1 3 122 120 80 grass poison ## 4 88.1 4 60 50 65 fire ## 5 88.1 5 80 65 80 fire ## 6 88.1 6 159 115 100 fire flying ## weight_kg generation is_legendary ## 1 6.9 1 0 ## 2 13.0 1 0 ## 3 100.0 1 0 ## 4 8.5 1 0 ## 5 19.0 1 0 ## 6 90.5 1 0 One slightly tricky thing to keep in mind about loading data is where R will look for the data you’re loading. This affects how you specify the file argument when you call read.csv. You can always specify the complete file path, for example (on my computer): pokemon_df &lt;- read.csv(file=&quot;/Users/lalejina/class_ws/CIS635-f22/gvsu-cis635-2022f/lecture-material/week-02/data/pokemon.csv&quot;) head(pokemon_df) ## abilities against_bug against_dark against_dragon ## 1 [&#39;Overgrow&#39;, &#39;Chlorophyll&#39;] 1.00 1 1 ## 2 [&#39;Overgrow&#39;, &#39;Chlorophyll&#39;] 1.00 1 1 ## 3 [&#39;Overgrow&#39;, &#39;Chlorophyll&#39;] 1.00 1 1 ## 4 [&#39;Blaze&#39;, &#39;Solar Power&#39;] 0.50 1 1 ## 5 [&#39;Blaze&#39;, &#39;Solar Power&#39;] 0.50 1 1 ## 6 [&#39;Blaze&#39;, &#39;Solar Power&#39;] 0.25 1 1 ## against_electric against_fairy against_fight against_fire against_flying ## 1 0.5 0.5 0.5 2.0 2 ## 2 0.5 0.5 0.5 2.0 2 ## 3 0.5 0.5 0.5 2.0 2 ## 4 1.0 0.5 1.0 0.5 1 ## 5 1.0 0.5 1.0 0.5 1 ## 6 2.0 0.5 0.5 0.5 1 ## against_ghost against_grass against_ground against_ice against_normal ## 1 1 0.25 1 2.0 1 ## 2 1 0.25 1 2.0 1 ## 3 1 0.25 1 2.0 1 ## 4 1 0.50 2 0.5 1 ## 5 1 0.50 2 0.5 1 ## 6 1 0.25 0 1.0 1 ## against_poison against_psychic against_rock against_steel against_water ## 1 1 2 1 1.0 0.5 ## 2 1 2 1 1.0 0.5 ## 3 1 2 1 1.0 0.5 ## 4 1 1 2 0.5 2.0 ## 5 1 1 2 0.5 2.0 ## 6 1 1 4 0.5 2.0 ## attack base_egg_steps base_happiness base_total capture_rate classfication ## 1 49 5120 70 318 45 Seed Pokémon ## 2 62 5120 70 405 45 Seed Pokémon ## 3 100 5120 70 625 45 Seed Pokémon ## 4 52 5120 70 309 45 Lizard Pokémon ## 5 64 5120 70 405 45 Flame Pokémon ## 6 104 5120 70 634 45 Flame Pokémon ## defense experience_growth height_m hp japanese_name name ## 1 49 1059860 0.7 45 Fushigidaneフシギダネ Bulbasaur ## 2 63 1059860 1.0 60 Fushigisouフシギソウ Ivysaur ## 3 123 1059860 2.0 80 Fushigibanaフシギバナ Venusaur ## 4 43 1059860 0.6 39 Hitokageヒトカゲ Charmander ## 5 58 1059860 1.1 58 Lizardoリザード Charmeleon ## 6 78 1059860 1.7 78 Lizardonリザードン Charizard ## percentage_male pokedex_number sp_attack sp_defense speed type1 type2 ## 1 88.1 1 65 65 45 grass poison ## 2 88.1 2 80 80 60 grass poison ## 3 88.1 3 122 120 80 grass poison ## 4 88.1 4 60 50 65 fire ## 5 88.1 5 80 65 80 fire ## 6 88.1 6 159 115 100 fire flying ## weight_kg generation is_legendary ## 1 6.9 1 0 ## 2 13.0 1 0 ## 3 100.0 1 0 ## 4 8.5 1 0 ## 5 19.0 1 0 ## 6 90.5 1 0 Often, it’ll be easier to specify a relative path. In RStudio, when you do not specify a complete path, paths are relative to your current working directory. Run getwd() to see what your current working directory is set to. Use setwd to change your working directory. Read more about project management and R working directories here: https://r4ds.had.co.nz/workflow-projects.html "],["tidyverse.html", "Chapter 7 tidyverse", " Chapter 7 tidyverse From the tidyverse website: The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. The tidyverse includes the following packages: ggplot2 - provides a system for creating graphics based on The Grammar of Graphics dplyr - provides functions for data manipulation tidyr - provides functions that help you turn your data into tidy data readr - provides fast functions for reading data from files forcats - provides functions for working with factors tibble - provides an alternative data structure (a tibble) to the data frame stringr - provides functions for working with strings purrr - provides tools for working with functions and vectors Install the tidyverse collection of packages using: install.packages(&quot;tidyverse&quot;) While all of the tidyverse is useful for working with data (and worth spending time to learn), we’ll focus on the dplyr, tidyr, and ggplot2 packages. "],["using-dplyr-to-manipulate-data.html", "Chapter 8 Using dplyr to manipulate data 8.1 Setup 8.2 Overview 8.3 The pipe operator 8.4 Data 8.5 Filter rows with filter() 8.6 Arrange rows with arrange() 8.7 Choose rows using their position with slice() 8.8 Select columns with select() 8.9 Rename columns with rename() 8.10 Add new columns with mutate() 8.11 Change column order with relocate() 8.12 Summarize groups of rows using summarise() 8.13 dplyr cheat sheet", " Chapter 8 Using dplyr to manipulate data You can download the R Markdown file used to generate this document (or webpage) here. I would encourage you to download that Rmd file, open it own your own computer in RStudio, and run the code chunks on your own (and modify things as you wish!). Goals: Learn about solving common data manipulation challenges with dplyr Learn to use the pipe operator (%&gt;%) Sources: Introduction to dplyr R for Data Science dplyr documentation Software carpentry lesson on dplyr 8.1 Setup You should have already installed the tidyverse collection of packages, which includes the dplyr package. But, if you haven’t, go ahead and run: install.packages(&quot;tidyverse&quot;) With the tidyverse packages installed, you’ll need to load the dplyr library to use it. library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union 8.2 Overview This document will cover key dplyr functions that solve the most common data manipulation challenges. The examples and explanations here non-exhaustive (dplyr has a lot of functionality and each of these functions are very flexible). Instead, I’d like you to start building a sense of the broad category of data manipulations that are possible with each of the dplyr functions below. As you encounter challenges with manipulating data, you should have a solid idea of which functions you will need to do what you want to do. Then, you can use those functions’ documentation to work out the specific details. dplyr’s functions for basic data manipulation can be divided into three categories based on the component of the dataset that they work with (descriptions pulled from dplyr documentation): Rows filter() - choose rows in your data based on their column values slice() - choose rows in your data based on their location (index) in the dataset arrange() - change the ordering of rows in your data Columns select() - changes whether or not a column is included rename() - changes the names of columns mutate() - changes the values of columns and can create new columns that are functions of existing columns relocate() - changes the order of columns Groups of rows summarise() - collapses a group of rows into a single row These functions can be used in combination with group_by(), which allows you to perform these functions on particular groups of rows, instead of the entire dataset. Note that the above list of functions is non-exhaustive. I refer you to the dplyr documentation for an exhaustive list. Additionally, I’d encourage you to look over (and save) the cheat sheet linked at the end of this document. 8.3 The pipe operator Most dplyr functions take a dataframe (or tibble) as their first argument. dplyr provides the pipe operator (%&gt;%), which lets you conveniently compose multiple dplyr functions in a single line (instead of needing to save intermediate states of your dataset). The pipe operator “pipes” the results of the left side of the pipe into the first argument of the right side of the pipe. Here’s an example: f &lt;- function(a, b) { return(a + b) } g &lt;- function(a, b) { return(a * b) } x &lt;- 2 y &lt;- 100 x %&gt;% f(y) # Is the same as f(x,y) ## [1] 102 x %&gt;% f(y) %&gt;% g(y) # Is the same as g(f(x,y), y) ## [1] 10200 8.4 Data We’ll use the starwars dataset to demonstrate the basic data manipulation functions provided by dplyr. You don’t need to download anything, the starwars dataset is loaded when you load the dplyr package. dim(starwars) ## [1] 87 14 This dataset has 87 rows and 14 columns where each row represents a character from Star Wars, and each column is a particular attribute. starwars ## # A tibble: 87 × 14 ## name height mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex gender homew…⁵ ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Skywa… 172 77 blond fair blue 19 male mascu… Tatooi… ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… Tatooi… ## 3 R2-D2 96 32 &lt;NA&gt; white,… red 33 none mascu… Naboo ## 4 Darth Vader 202 136 none white yellow 41.9 male mascu… Tatooi… ## 5 Leia Organa 150 49 brown light brown 19 fema… femin… Aldera… ## 6 Owen Lars 178 120 brown,… light blue 52 male mascu… Tatooi… ## 7 Beru White… 165 75 brown light blue 47 fema… femin… Tatooi… ## 8 R5-D4 97 32 &lt;NA&gt; white,… red NA none mascu… Tatooi… ## 9 Biggs Dark… 183 84 black light brown 24 male mascu… Tatooi… ## 10 Obi-Wan Ke… 182 77 auburn… fair blue-g… 57 male mascu… Stewjon ## # … with 77 more rows, 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt;, and abbreviated variable names ## # ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld If you look closely at the above output, you’ll notice that the starwars dataset is represented as a tibble. A tibble is a more modern version of the dataframe type, and for the most part, anywhere you could use a dataframe, you can use a tibble. You can convert a data frame to tibbles with the as_tible() function. Read more about tibbles here: https://tibble.tidyverse.org/. 8.5 Filter rows with filter() filter() allows you to choose rows in a data frame based on their column values. For example, if we wanted to create a new data frame that only contained droids, we could: starwars %&gt;% filter(species==&quot;Droid&quot;) ## # A tibble: 6 × 14 ## name height mass hair_color skin_color eye_c…¹ birth…² sex gender homew…³ ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… Tatooi… ## 2 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… Naboo ## 3 R5-D4 97 32 &lt;NA&gt; white, red red NA none mascu… Tatooi… ## 4 IG-88 200 140 none metal red 15 none mascu… &lt;NA&gt; ## 5 R4-P17 96 NA none silver, r… red, b… NA none femin… &lt;NA&gt; ## 6 BB8 NA NA none none black NA none mascu… &lt;NA&gt; ## # … with 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, ## # starships &lt;list&gt;, and abbreviated variable names ¹​eye_color, ²​birth_year, ## # ³​homeworld We can also filter rows based on more complex conditions: starwars %&gt;% filter(species == &quot;Droid&quot; &amp; height &gt; 100) ## # A tibble: 2 × 14 ## name height mass hair_color skin_color eye_co…¹ birth…² sex gender homew…³ ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… Tatooi… ## 2 IG-88 200 140 none metal red 15 none mascu… &lt;NA&gt; ## # … with 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, ## # starships &lt;list&gt;, and abbreviated variable names ¹​eye_color, ²​birth_year, ## # ³​homeworld What if we wanted all droids, Wookiees, and Hutts? starwars %&gt;% filter(species == &quot;Droid&quot; | species == &quot;Wookiee&quot; | species == &quot;Hutt&quot;) ## # A tibble: 9 × 14 ## name height mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex gender homew…⁵ ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… Tatooi… ## 2 R2-D2 96 32 &lt;NA&gt; white,… red 33 none mascu… Naboo ## 3 R5-D4 97 32 &lt;NA&gt; white,… red NA none mascu… Tatooi… ## 4 Chewbacca 228 112 brown unknown blue 200 male mascu… Kashyy… ## 5 Jabba Desil… 175 1358 &lt;NA&gt; green-… orange 600 herm… mascu… Nal Hu… ## 6 IG-88 200 140 none metal red 15 none mascu… &lt;NA&gt; ## 7 R4-P17 96 NA none silver… red, b… NA none femin… &lt;NA&gt; ## 8 Tarfful 234 136 brown brown blue NA male mascu… Kashyy… ## 9 BB8 NA NA none none black NA none mascu… &lt;NA&gt; ## # … with 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, ## # starships &lt;list&gt;, and abbreviated variable names ¹​hair_color, ²​skin_color, ## # ³​eye_color, ⁴​birth_year, ⁵​homeworld Even more simply with the %in% operator, starwars %&gt;% filter(species %in% c(&quot;Droid&quot;, &quot;Wookiee&quot;, &quot;Hutt&quot;)) ## # A tibble: 9 × 14 ## name height mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex gender homew…⁵ ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… Tatooi… ## 2 R2-D2 96 32 &lt;NA&gt; white,… red 33 none mascu… Naboo ## 3 R5-D4 97 32 &lt;NA&gt; white,… red NA none mascu… Tatooi… ## 4 Chewbacca 228 112 brown unknown blue 200 male mascu… Kashyy… ## 5 Jabba Desil… 175 1358 &lt;NA&gt; green-… orange 600 herm… mascu… Nal Hu… ## 6 IG-88 200 140 none metal red 15 none mascu… &lt;NA&gt; ## 7 R4-P17 96 NA none silver… red, b… NA none femin… &lt;NA&gt; ## 8 Tarfful 234 136 brown brown blue NA male mascu… Kashyy… ## 9 BB8 NA NA none none black NA none mascu… &lt;NA&gt; ## # … with 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, ## # starships &lt;list&gt;, and abbreviated variable names ¹​hair_color, ²​skin_color, ## # ³​eye_color, ⁴​birth_year, ⁵​homeworld Notice that the original starwars dataset is never modified by any of the filter functions. None of the dplyr functions modify the dataset passed to them with the data argument. Instead, they return a new data frame with the appropriate manipulations. 8.6 Arrange rows with arrange() arrange() reorders rows in the given data frame. It takes a data frame and a set of columns names to reorder by. If you give more than one column name, additional column names are used to break ties in the values of proceeding column names. starwars %&gt;% arrange(mass, height) ## # A tibble: 87 × 14 ## name height mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex gender homew…⁵ ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Ratts Tyer… 79 15 none grey, … unknown NA male mascu… Aleen … ## 2 Yoda 66 17 white green brown 896 male mascu… &lt;NA&gt; ## 3 Wicket Sys… 88 20 brown brown brown 8 male mascu… Endor ## 4 R2-D2 96 32 &lt;NA&gt; white,… red 33 none mascu… Naboo ## 5 R5-D4 97 32 &lt;NA&gt; white,… red NA none mascu… Tatooi… ## 6 Sebulba 112 40 none grey, … orange NA male mascu… Malast… ## 7 Dud Bolt 94 45 none blue, … yellow NA male mascu… Vulpter ## 8 Padmé Amid… 165 45 brown light brown 46 fema… femin… Naboo ## 9 Sly Moore 178 48 none pale white NA &lt;NA&gt; &lt;NA&gt; Umbara ## 10 Wat Tambor 193 48 none green,… unknown NA male mascu… Skako ## # … with 77 more rows, 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt;, and abbreviated variable names ## # ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld You can use desc() to order in descending order. starwars %&gt;% arrange(desc(mass)) ## # A tibble: 87 × 14 ## name height mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex gender homew…⁵ ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Jabba Desi… 175 1358 &lt;NA&gt; green-… orange 600 herm… mascu… Nal Hu… ## 2 Grievous 216 159 none brown,… green,… NA male mascu… Kalee ## 3 IG-88 200 140 none metal red 15 none mascu… &lt;NA&gt; ## 4 Darth Vader 202 136 none white yellow 41.9 male mascu… Tatooi… ## 5 Tarfful 234 136 brown brown blue NA male mascu… Kashyy… ## 6 Owen Lars 178 120 brown,… light blue 52 male mascu… Tatooi… ## 7 Bossk 190 113 none green red 53 male mascu… Trando… ## 8 Chewbacca 228 112 brown unknown blue 200 male mascu… Kashyy… ## 9 Jek Tono P… 180 110 brown fair blue NA male mascu… Bestin… ## 10 Dexter Jet… 198 102 none brown yellow NA male mascu… Ojom ## # … with 77 more rows, 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt;, and abbreviated variable names ## # ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld 8.7 Choose rows using their position with slice() Use slice to choose rows by their position. For example, to choose the character in the second row of the data frame: starwars %&gt;% slice(2) ## # A tibble: 1 × 14 ## name height mass hair_color skin_color eye_co…¹ birth…² sex gender homew…³ ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… Tatooi… ## # … with 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, ## # starships &lt;list&gt;, and abbreviated variable names ¹​eye_color, ²​birth_year, ## # ³​homeworld The first, tenth, and twentieth rows of the data frame, starwars %&gt;% slice(c(1,10,20)) ## # A tibble: 3 × 14 ## name height mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex gender homew…⁵ ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Skywal… 172 77 blond fair blue 19 male mascu… Tatooi… ## 2 Obi-Wan Ken… 182 77 auburn… fair blue-g… 57 male mascu… Stewjon ## 3 Palpatine 170 75 grey pale yellow 82 male mascu… Naboo ## # … with 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, ## # starships &lt;list&gt;, and abbreviated variable names ¹​hair_color, ²​skin_color, ## # ³​eye_color, ⁴​birth_year, ⁵​homeworld The odd rows of the data frame, starwars %&gt;% slice(seq(1,nrow(starwars), 2)) ## # A tibble: 44 × 14 ## name height mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex gender homew…⁵ ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Skywa… 172 77 blond fair blue 19 male mascu… Tatooi… ## 2 R2-D2 96 32 &lt;NA&gt; white,… red 33 none mascu… Naboo ## 3 Leia Organa 150 49 brown light brown 19 fema… femin… Aldera… ## 4 Beru White… 165 75 brown light blue 47 fema… femin… Tatooi… ## 5 Biggs Dark… 183 84 black light brown 24 male mascu… Tatooi… ## 6 Anakin Sky… 188 84 blond fair blue 41.9 male mascu… Tatooi… ## 7 Chewbacca 228 112 brown unknown blue 200 male mascu… Kashyy… ## 8 Greedo 173 74 &lt;NA&gt; green black 44 male mascu… Rodia ## 9 Wedge Anti… 170 77 brown fair hazel 21 male mascu… Corell… ## 10 Yoda 66 17 white green brown 896 male mascu… &lt;NA&gt; ## # … with 34 more rows, 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt;, and abbreviated variable names ## # ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld Combine with arrange() to quickly get the character with the largest height, starwars %&gt;% arrange(desc(height)) %&gt;% slice(1) ## # A tibble: 1 × 14 ## name height mass hair_c…¹ skin_…² eye_c…³ birth…⁴ sex gender homew…⁵ ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Yarael Poof 264 NA none white yellow NA male mascu… Quermia ## # … with 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, ## # starships &lt;list&gt;, and abbreviated variable names ¹​hair_color, ²​skin_color, ## # ³​eye_color, ⁴​birth_year, ⁵​homeworld There are a few convenient helper versions of the slice function: slice_head slice_tail slice_sample slice_min slice_max I’ll leave it to you to pull up the help page for those functions. 8.8 Select columns with select() When you’re working with large datasets, sometimes you want to drop all of the columns you’re not using or interested in. For example, if you only wanted the species column: starwars %&gt;% select(species) ## # A tibble: 87 × 1 ## species ## &lt;chr&gt; ## 1 Human ## 2 Droid ## 3 Droid ## 4 Human ## 5 Human ## 6 Human ## 7 Human ## 8 Droid ## 9 Human ## 10 Human ## # … with 77 more rows Or, if you wanted name, species, and homeworld: starwars %&gt;% select(name, species, homeworld) ## # A tibble: 87 × 3 ## name species homeworld ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Skywalker Human Tatooine ## 2 C-3PO Droid Tatooine ## 3 R2-D2 Droid Naboo ## 4 Darth Vader Human Tatooine ## 5 Leia Organa Human Alderaan ## 6 Owen Lars Human Tatooine ## 7 Beru Whitesun lars Human Tatooine ## 8 R5-D4 Droid Tatooine ## 9 Biggs Darklighter Human Tatooine ## 10 Obi-Wan Kenobi Human Stewjon ## # … with 77 more rows What if you wanted all columns with the letter m in the column name? starwars %&gt;% select(contains(&quot;m&quot;)) ## # A tibble: 87 × 4 ## name mass homeworld films ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;list&gt; ## 1 Luke Skywalker 77 Tatooine &lt;chr [5]&gt; ## 2 C-3PO 75 Tatooine &lt;chr [6]&gt; ## 3 R2-D2 32 Naboo &lt;chr [7]&gt; ## 4 Darth Vader 136 Tatooine &lt;chr [4]&gt; ## 5 Leia Organa 49 Alderaan &lt;chr [5]&gt; ## 6 Owen Lars 120 Tatooine &lt;chr [3]&gt; ## 7 Beru Whitesun lars 75 Tatooine &lt;chr [3]&gt; ## 8 R5-D4 32 Tatooine &lt;chr [1]&gt; ## 9 Biggs Darklighter 84 Tatooine &lt;chr [1]&gt; ## 10 Obi-Wan Kenobi 77 Stewjon &lt;chr [6]&gt; ## # … with 77 more rows See the tidyselect documentation for more selection helpers (like contains). 8.9 Rename columns with rename() rename() lets you rename columns. For example, if you wanted to rename the column “homeworld” to “home_world”: renamed_starwars &lt;- starwars %&gt;% rename(home_world=homeworld) colnames(starwars) ## [1] &quot;name&quot; &quot;height&quot; &quot;mass&quot; &quot;hair_color&quot; &quot;skin_color&quot; ## [6] &quot;eye_color&quot; &quot;birth_year&quot; &quot;sex&quot; &quot;gender&quot; &quot;homeworld&quot; ## [11] &quot;species&quot; &quot;films&quot; &quot;vehicles&quot; &quot;starships&quot; colnames(renamed_starwars) ## [1] &quot;name&quot; &quot;height&quot; &quot;mass&quot; &quot;hair_color&quot; &quot;skin_color&quot; ## [6] &quot;eye_color&quot; &quot;birth_year&quot; &quot;sex&quot; &quot;gender&quot; &quot;home_world&quot; ## [11] &quot;species&quot; &quot;films&quot; &quot;vehicles&quot; &quot;starships&quot; 8.10 Add new columns with mutate() Often, it is useful to add new columns that are functions of existing columns. For example, we could add a new column to each row that is the product of that row’s height and mass values. starwars_new_col &lt;- starwars %&gt;% mutate(heightXmass = height * mass) colnames(starwars_new_col) ## [1] &quot;name&quot; &quot;height&quot; &quot;mass&quot; &quot;hair_color&quot; &quot;skin_color&quot; ## [6] &quot;eye_color&quot; &quot;birth_year&quot; &quot;sex&quot; &quot;gender&quot; &quot;homeworld&quot; ## [11] &quot;species&quot; &quot;films&quot; &quot;vehicles&quot; &quot;starships&quot; &quot;heightXmass&quot; 8.11 Change column order with relocate() Relocate let’s you move columns around in your data set. For example, to move the name column after the height column, you can: starwars %&gt;% relocate(name, .after=height) ## # A tibble: 87 × 14 ## height name mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex gender homew…⁵ ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 172 Luke Skywa… 77 blond fair blue 19 male mascu… Tatooi… ## 2 167 C-3PO 75 &lt;NA&gt; gold yellow 112 none mascu… Tatooi… ## 3 96 R2-D2 32 &lt;NA&gt; white,… red 33 none mascu… Naboo ## 4 202 Darth Vader 136 none white yellow 41.9 male mascu… Tatooi… ## 5 150 Leia Organa 49 brown light brown 19 fema… femin… Aldera… ## 6 178 Owen Lars 120 brown,… light blue 52 male mascu… Tatooi… ## 7 165 Beru White… 75 brown light blue 47 fema… femin… Tatooi… ## 8 97 R5-D4 32 &lt;NA&gt; white,… red NA none mascu… Tatooi… ## 9 183 Biggs Dark… 84 black light brown 24 male mascu… Tatooi… ## 10 182 Obi-Wan Ke… 77 auburn… fair blue-g… 57 male mascu… Stewjon ## # … with 77 more rows, 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt;, and abbreviated variable names ## # ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld 8.12 Summarize groups of rows using summarise() summarise() collapses a dataset into a single row. starwars %&gt;% summarise(mass = mean(mass, na.rm=TRUE)) ## # A tibble: 1 × 1 ## mass ## &lt;dbl&gt; ## 1 97.3 summarise() is particularly useful in combination with the group_by function. For example, to get the average mass of characters in each species, starwars %&gt;% group_by(species) %&gt;% summarise(mass=mean(mass, na.rm=TRUE)) ## # A tibble: 38 × 2 ## species mass ## &lt;chr&gt; &lt;dbl&gt; ## 1 Aleena 15 ## 2 Besalisk 102 ## 3 Cerean 82 ## 4 Chagrian NaN ## 5 Clawdite 55 ## 6 Droid 69.8 ## 7 Dug 40 ## 8 Ewok 20 ## 9 Geonosian 80 ## 10 Gungan 74 ## # … with 28 more rows Or, if we wanted to count the number of characters of each species, starwars %&gt;% group_by(species) %&gt;% summarise(count=n()) ## # A tibble: 38 × 2 ## species count ## &lt;chr&gt; &lt;int&gt; ## 1 Aleena 1 ## 2 Besalisk 1 ## 3 Cerean 1 ## 4 Chagrian 1 ## 5 Clawdite 1 ## 6 Droid 6 ## 7 Dug 1 ## 8 Ewok 1 ## 9 Geonosian 1 ## 10 Gungan 3 ## # … with 28 more rows 8.13 dplyr cheat sheet Download pdf here: https://github.com/rstudio/cheatsheets/blob/main/data-transformation.pdf "],["using-tidyr-to-create-tidy-data.html", "Chapter 9 Using tidyr to create tidy data 9.1 Setup 9.2 Overview 9.3 What is tidy data? 9.4 Making data tidy", " Chapter 9 Using tidyr to create tidy data You can download the R Markdown file used to generate this document (or webpage) here. I would encourage you to download that Rmd file, open it own your own computer in RStudio, and run the code chunks on your own (and modify things as you wish!). Goals: Work through examples of using tidyr (and dplyr) functions to clean up messy (un-tidy) data Sources: https://tidyr.tidyverse.org/articles/tidy-data.html R for Data Science 9.1 Setup You should have already installed the tidyverse collection of packages, which includes the tidyr package. But, if you haven’t, go ahead and run: install.packages(&quot;tidyverse&quot;) In this document, we’ll be using the tidyr and dplyr packages. library(tidyr) library(dplyr) 9.2 Overview If you have not already, I suggest that you read the following article for an introduction to tidy data: https://tidyr.tidyverse.org/articles/tidy-data.html. Additionally, I suggest that you work through the dplyr R Markdown material before working through this document. The tidyr package provides functions for making messy data “tidy”. For a complete list of the included functions, see https://tidyr.tidyverse.org/reference/index.html. Understanding tidy data and how to reformat your data into a tidy format is very important, as many of the functions/packages that we will use this semester are written to work with tidy-format data. For example, ggplot (the R graphics package that we will be using) assumes tidy data. Taking the time now to get comfortable with what it means to have tidy data will save you substantial time later on. 9.3 What is tidy data? You can represent the same underlying data many ways. For example, let’s say we have the following data about students’ grades: messy_grade_data1 &lt;- read.csv(&quot;lecture-material/week-02/data/messy_grades_1.csv&quot;, na.strings=&quot;&quot;) messy_grade_data1 ## name quiz1 quiz2 test1 ## 1 Billy &lt;NA&gt; D C ## 2 Suzy F &lt;NA&gt; &lt;NA&gt; ## 3 Lionel B C B ## 4 Jenny A A B Or, the same exact data could be structured as follows: messy_grade_data2 &lt;- read.csv(&quot;lecture-material/week-02/data/messy_grades_2.csv&quot;, na.strings=&quot;&quot;) messy_grade_data2 ## assessment Billy Suzy Lionel Jenny ## 1 quiz1 &lt;NA&gt; FALSE B A ## 2 quiz2 D NA C A ## 3 test1 C NA B B Neither of these formats are particularly convenient to work with. For example, in messy_grade_data2 format, it would be difficult to query the full set of student names in the dataset (because each student has their own column). Likewise, in messy_grade_data1, it would be diffult to query the set of assessments. From https://tidyr.tidyverse.org/articles/tidy-data.html, A dataset is a collection of values, usually either numbers (if quantitative) or strings (if qualitative). Values are organised in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes. A tidy version of the grade data would look something like this: tidy_grade_data &lt;- messy_grade_data1 %&gt;% pivot_longer(c(quiz1,quiz2,test1), names_to=&quot;assessment&quot;, values_to=&quot;grade&quot;) tidy_grade_data ## # A tibble: 12 × 3 ## name assessment grade ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Billy quiz1 &lt;NA&gt; ## 2 Billy quiz2 D ## 3 Billy test1 C ## 4 Suzy quiz1 F ## 5 Suzy quiz2 &lt;NA&gt; ## 6 Suzy test1 &lt;NA&gt; ## 7 Lionel quiz1 B ## 8 Lionel quiz2 C ## 9 Lionel test1 B ## 10 Jenny quiz1 A ## 11 Jenny quiz2 A ## 12 Jenny test1 B This arrangement makes the values, variables, and observations clearer. What are the values, variables, and observations in the grade data? (the answer is here). A dataset is tidy if: Each variable has its own column Each observation has its own row Each value has its own cell In the grade data, we have the following variables: student name, assessment, and grade. Each observation is a particular student’s grade on a particular assessment (e.g., Jenny’s grade on test1). Each cell in the dataset specifies a value for a particular variable (e.g., in row 1 of tidy_grade_data, Billy is the value of the name variable). 9.4 Making data tidy Two common ways that data is messy: Column headers are values, not variable names Multiple variables are stored in one column For more common ways that data are often messy, see: https://tidyr.tidyverse.org/articles/tidy-data.html#tidying Below, are datasets that exemplify each problem. 9.4.1 Column headers are values, not variables Observations over time are often recorded in a “wide” format. That is, each timepoint that an observation is made is entered as a column. This is convienient for data entry. For example, imagine you measure the temperature of a lake at several different depths over a 10 week period (one measurement for each depth every week). It’s convenient to enter that data in a “wide” format where each week is a column. However, this is not tidy, as each row is no longer a single observation. The billboard dataset that is loaded when you load the tidyr package is exemplifies this problem. billboard ## # A tibble: 317 × 79 ## artist track date.ent…¹ wk1 wk2 wk3 wk4 wk5 wk6 wk7 wk8 wk9 ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 Pac Baby… 2000-02-26 87 82 72 77 87 94 99 NA NA ## 2 2Ge+h… The … 2000-09-02 91 87 92 NA NA NA NA NA NA ## 3 3 Doo… Kryp… 2000-04-08 81 70 68 67 66 57 54 53 51 ## 4 3 Doo… Loser 2000-10-21 76 76 72 69 67 65 55 59 62 ## 5 504 B… Wobb… 2000-04-15 57 34 25 17 17 31 36 49 53 ## 6 98^0 Give… 2000-08-19 51 39 34 26 26 19 2 2 3 ## 7 A*Tee… Danc… 2000-07-08 97 97 96 95 100 NA NA NA NA ## 8 Aaliy… I Do… 2000-01-29 84 62 51 41 38 35 35 38 38 ## 9 Aaliy… Try … 2000-03-18 59 53 38 28 21 18 16 14 12 ## 10 Adams… Open… 2000-08-26 76 76 74 69 68 67 61 58 57 ## # … with 307 more rows, 67 more variables: wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;, ## # wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;, ## # wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;, ## # wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;, ## # wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;, ## # wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, ## # wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, … The billboard dataset records the date a song enters the billboard top 100. First on your own, use the pivot_longer function to create a tidy version of the billboard dataset. The solution can be found here. 9.4.2 Multiple variables are stored in one column Sometimes, the values in a column describe multiple variables. For example, in the following messy dataset, the month and day of each temperature recording are combined under a single column. messy_temperature &lt;- read.csv(&quot;lecture-material/week-02/data/multi_variables_single_col.csv&quot;) messy_temperature ## date year temperature ## 1 jan1 2022 15 ## 2 jan14 2022 130 ## 3 feb1 2022 19 ## 4 feb14 2022 85 ## 5 mar1 2022 95 ## 6 mar14 2022 130 ## 7 apr1 2022 29 ## 8 apr14 2022 42 ## 9 may1 2022 138 ## 10 may14 2022 14 ## 11 jun1 2022 133 ## 12 jun14 2022 53 ## 13 jul1 2022 120 ## 14 jul14 2022 108 ## 15 aug1 2022 23 ## 16 aug14 2022 148 ## 17 sep1 2022 103 ## 18 sep14 2022 113 ## 19 nov1 2022 67 ## 20 nov14 2022 1 ## 21 oct1 2022 113 ## 22 oct14 2022 56 ## 23 dec1 2022 107 ## 24 dec14 2022 30 The first three characters of the “date” column give the month, and the following numeric value gives the day of the month of the temperature reading. We can use the separate function to properly split the date column into multiple columns. tidy_temperature &lt;- messy_temperature %&gt;% separate(col=date, into=c(&quot;month&quot;,&quot;day&quot;), sep=3) tidy_temperature ## month day year temperature ## 1 jan 1 2022 15 ## 2 jan 14 2022 130 ## 3 feb 1 2022 19 ## 4 feb 14 2022 85 ## 5 mar 1 2022 95 ## 6 mar 14 2022 130 ## 7 apr 1 2022 29 ## 8 apr 14 2022 42 ## 9 may 1 2022 138 ## 10 may 14 2022 14 ## 11 jun 1 2022 133 ## 12 jun 14 2022 53 ## 13 jul 1 2022 120 ## 14 jul 14 2022 108 ## 15 aug 1 2022 23 ## 16 aug 14 2022 148 ## 17 sep 1 2022 103 ## 18 sep 14 2022 113 ## 19 nov 1 2022 67 ## 20 nov 14 2022 1 ## 21 oct 1 2022 113 ## 22 oct 14 2022 56 ## 23 dec 1 2022 107 ## 24 dec 14 2022 30 Run ?separate to see how the separate function works. Adding extra columns doesn’t “untidy” your data. Sometimes it is useful to add additional aggregate labels to an observation (especially when working with dates). For example, it might be useful to add a proper date column (with the format YEAR-MONTH-DAY) to our tidied temperature data. I’ll leave that as an exercise for you to do on your own. You’ll need to map each month string to its numeric representation (e.g. jan = 1, feb = 2). Then you can use mutate to create a new column from the relevant columns. "],["creating-graphics-with-ggplot2.html", "Chapter 10 Creating graphics with ggplot2 10.1 Resources for using ggplot2", " Chapter 10 Creating graphics with ggplot2 Read the Data Visualization chapter of R for Data Science. the tidyverse collection of packages, which includes the ggplot2 package. But, if you haven’t, go ahead and run: install.packages(&quot;tidyverse&quot;) 10.1 Resources for using ggplot2 For a super handy ggplot2 reference: ggplot2 cheatsheet Software carpentry lesson on creating publication quality graphics with ggplot2 ggplot2 documentation R for Data Science "],["homework-2.html", "Chapter 11 Homework 2 11.1 Overview 11.2 Setup 11.3 Part A. Loading data in R 11.4 Part B. Visualizing data with ggplot2", " Chapter 11 Homework 2 11.1 Overview 11.1.1 Objectives Identify a public dataset (formatted as a .csv) Practice using R to load and view characteristics of a dataset Practice using ggplot2 to create visualizations 11.1.2 Grading Part A is worth 40% Part B is worth 60% 11.2 Setup Create a new R Markdown file with the title “Homework 2” and with you as the author (hint: this information should be in your frontmatter at the top of the file). For this assignment, you’ll need to have the tidyverse collection of R packages installed. If you haven’t already, go ahead and install them by running install.packages(\"tidyverse\"). In your R markdown file, create a section heading for each of the following parts of your homework: Part A. Visualizing data with ggplot2 Part B. Loading data in R 11.3 Part A. Loading data in R Find a publicly available dataset that is represented as a .csv file. Your chosen dataset should not be trivial (# rows * # columns should be at least 250 cells), and you may not choose the same dataset that I demonstrate in this week’s lecture material. When choosing a dataset, keep in mind that you need to creating at least three different types of plots using your chosen dataset. You will upload the chosen data set as a .csv file along with your homework submission. Tell me about your data set. Where did you get it? Who/what created it? Load your dataset into a data frame (or a tibble, your choice) Print the number of rows and columns (using R code) Print the column names (using R) If your dataset is not tidy, reformat it (using R code) to be tidy. Use the mutate function (from the dplyr package) to add a column to your data frame. It’s okay if the new column isn’t useful. 11.4 Part B. Visualizing data with ggplot2 Using ggplot2, create three different types of plots (use at least three different geoms) using your chosen data set from Part A. Be creative! Feel free to draw inspiration from the R Graph Gallery. On each of your plots, explicitly label your axes (e.g., using the labs function). On at least one of your plots, use either the color or fill aesthetic. "],["homework-3.html", "Chapter 12 Homework 3 12.1 Overview 12.2 Setup 12.3 Part A. Attribute descriptions 12.4 Part B. R practice 12.5 Part C. Thinking about common data mining applications", " Chapter 12 Homework 3 12.1 Overview 12.1.1 Objectives Identify nominal versus ordinal attributes Practice loading a dataset and using ggplot to graph data Consider general data mining tasks 12.1.2 Grading Uploaded all requested files, 5% R markdown (and compiled pdf) are properly formatted, 5% Proper section headers for each part of your homework. You clearly indicate which question each of your responses are associated with. Part A., 30% Part B., 10% Part C., 50% 12.1.3 Deliverables .pdf file (generated by knitting your .rmd file) .rmd file (used to generate the pdf file) 12.2 Setup Create a new R Markdown file with the title “Homework 3” and with you as the author (hint: this information should be in your frontmatter at the top of the file). In your R Markdown file, create a section heading for each of the following parts of your homework (see this documentation for how to make a heading): Part A Part B Part C When you knit your markdown document, these headings should be actual headings (like they are in this document). For each of your responses, clearly indicate which question the responses belongs to. 12.3 Part A. Attribute descriptions You are given a data file (see assignment attachments on blackboard): hw03data.csv. This is a table of ratings for teaching assistants (TA) at some university (this is modified data that originally came from https://archive.ics.uci.edu/ml/datasets.php). There are 6 attributes: id: unique number to identify each rating instructor: id number that identifies the instructor classNbr: id number that identifies the course semester: 1=summer, 2=fall, 3=winter classSize: number of students in the class rating: 1=low, 2=medium, 3=high For each attribute, describe whether it is nominal or ordinal. id instructor classNbr semester classSize rating 12.4 Part B. R practice Load the tidyverse collection of packages and load hw03data.csv in R (as a dataframe or tibble). Using ggplot2, graph the relationship between class size and rating. The type of graph is up to you (but must be appropriate). Explicitly label your graph axes (e.g., using the labs function). 12.5 Part C. Thinking about common data mining applications This question asks you to think about clustering. Considering an average kitchen, identify 4 clusters of objects. Name them according to their utility. For example, if I were asked to identify 4 clusters in a sporting goods store, I might choose apparel (e.g., shoes, shirts), containers (e.g., backpacks, coolers), sports equipment (e.g., basketball, tennis racquets), and fishing (e.g., rods, lures). Pick one of the following forms of predictive modeling: regression, classification, or ranking. Describe an application domain or a dataset where you think it would be useful to apply the chosen predictive modeling approach. What attribute(s) would you be predicting? What attributes would you use to make the prediction? "],["simple-denoising-examples.html", "Chapter 13 Simple denoising examples 13.1 Generating the ground truth 13.2 Adding some noise 13.3 Denoising using a rolling average 13.4 Denoising using a median filter 13.5 Further exploration", " Chapter 13 Simple denoising examples library(tidyverse) ## ── Attaching packages ──────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.3.6 ✔ purrr 0.3.4 ## ✔ tibble 3.1.8 ✔ stringr 1.4.1 ## ✔ readr 2.1.2 ✔ forcats 0.5.2 ## ── Conflicts ─────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() In this exercise: First, we’ll generate a time series of “measurements” where the process we are measuring behaves exactly like a sine wave. This will be our “ground truth” signal. Next, we’ll simulate different types of noise that could result from imperfect measurements. Finally, we’ll apply 2 basic denoising techniques to each of our noisy signals. 13.1 Generating the ground truth For this demonstration, we’ll generate a signal that behaves like a sine wave. Feel free to adjust any of the parameters to see how it influences our ability to denoise the signal. # Resolution gives the number of measurements in our signal. resolution &lt;- 1000 # x_range_min gives the minimum x value in our signal. x_range_min &lt;- 0 # x_range_max gives the maximum x value in our signal. x_range_max &lt;- (8 * pi) # Generate our signal x,y: x &lt;- (seq(from=x_range_min, to=resolution) / resolution) * x_range_max y &lt;- sin(x) # Create a data frame data &lt;- data.frame(x=x, y_true=y) data$y_true gives the ground truth signal. We can graph our ground truth: ggplot(data, aes(x = x, y = y_true)) + geom_point() + labs( x = &quot;X&quot;, y = &quot;True value&quot;, title = &quot;Ground truth&quot; ) 13.2 Adding some noise First, let’s simulate that we had a noisy sensor such that each measurement has a small amount of random noise. To do so, we’ll add a little bit of Gaussian noise to our ground truth signal. # noise_sd controls the level of noise applied to each measurement noise_sd &lt;- 0.1 data$y_noise &lt;- data$y + rnorm(length(data$y), 0, 0.1) We can graph this noisy signal: ggplot(data, aes(x = x, y = y_noise)) + geom_point() + labs( x = &quot;X&quot;, y = &quot;Noisy measurement value&quot; ) Next, let’s add another kind of noise that we might encounter with our sensor. What if our sensor randomly fails, returning a nonsense value (in this case, let’s say 99). prob_sensor_failure &lt;- 0.02 # Sensor fails 2% of the time failure_value &lt;- 99 # &quot;Simulate&quot; when our sensor fails. Use rbinom to flip a biased coin for each object in our data. data$sensor_failure &lt;- rbinom(nrow(data), 1, prob_sensor_failure) data$y_noise_faulty &lt;- mapply( function(y, fail) { if (fail) { return(failure_value) } else { return (y) } }, data$y_noise, data$sensor_failure ) Graph our faulty sensor data. ggplot(data, aes(x = x, y = y_noise_faulty)) + geom_point() + labs( x = &quot;X&quot;, y = &quot;Faulty measurement value&quot; ) We have our nice signal, and then we have those sensor failure measurements throwing everything off. 13.3 Denoising using a rolling average Let’s try denoising using a rolling average. This technique loops over each value in your sequence, setting that value to the average of its neighbors. The number of neighbors is determined by the “window size”. # This function calculates a rolling average over the sequence parameter. # window_width defines how far the window extends to either side of the each index. rolling_avg &lt;- function(sequence, window_width=1) { avgs &lt;- c() for (i in seq_along(sequence)) { window &lt;- c(i) for (w_i in 1:window_width) { left &lt;- i - w_i right &lt;- i + w_i # Check that our window doesn&#39;t walk off the edge of our sequence. # If so, extend the other side of the window. if (left &lt; 1) { left &lt;- (i + window_width) + (abs(1 - left)) } if (right &gt; length(sequence)) { right &lt;- (i - window_width) - ((right - length(sequence)) ) } window &lt;- c(window, left) window &lt;- c(window, right) } avgs &lt;- c(avgs, mean( sequence[window] )) } return(avgs) } Apply our rolling average function to y_noise window_w &lt;- 10 data$y_noise_denoised &lt;- rolling_avg( data$y_noise, window_width=window_w ) ggplot(data, aes(x=x, y=y_noise_denoised)) + geom_point() Much smoother! The rolling average works well when you want to smooth out a small magnitude of random noise. Apply our rolling average function to y_noise_faulty: window_w &lt;- 10 data$y_noise_faulty_denoised &lt;- rolling_avg( data$y_noise_faulty, window_width=window_w ) ggplot(data, aes(x=x, y=y_noise_faulty_denoised)) + geom_point() Yikes! Our signal looks worse than it did before we applied the rolling average denoising. 13.3.1 Exercises Generate a small sequence of 10 random values. On paper, apply a rolling average filter to your sequence. You decide the window size. What would happen if you increased your window size? What about if you decreased your window size? Is the rolling average a good denoising approach for y_noise? What about y_noise_faulty? Did the rolling average improve how representative y_noise_faulty is of the ground truth signal? Try calculating the mean squared error for each of y_noise and y_noise_denoised. Does error go down? What about for y_noise_faulty and y_noise_faulty_denoised? What happens when you make the rolling average window smaller? Bigger? What happens when you add more noise to the signal? Identify any lines of code that you don’t understand. Use the documentation to figure out what those lines of code are doing. 13.4 Denoising using a median filter A median filter is similar to applying a rolling average, except instead of replacing each value in our sequence with the average of its neighbors, we replace it with the median of its neighbors. # This function calculates a rolling average over the sequence parameter. # window_width defines how far the window extends to either side of the each index. median_filter &lt;- function(sequence, window_width=1) { avgs &lt;- c() for (i in seq_along(sequence)) { window &lt;- c(i) for (w_i in 1:window_width) { left &lt;- i - w_i right &lt;- i + w_i # Check that our window doesn&#39;t walk off the edge of our sequence. # If so, extend the other side of the window. if (left &lt; 1) { left &lt;- (i + window_width) + (abs(1 - left)) } if (right &gt; length(sequence)) { right &lt;- (i - window_width) - ((right - length(sequence)) ) } window &lt;- c(window, left) window &lt;- c(window, right) } avgs &lt;- c(avgs, median( sequence[window] )) } return(avgs) } Apply our rolling average function to y_noise: window_w &lt;- 10 data$y_noise_denoised &lt;- median_filter( data$y_noise, window_width=window_w ) ggplot(data, aes(x=x, y=y_noise_denoised)) + geom_point() Apply our rolling average function to y_noise_faulty: window_w &lt;- 10 data$y_noise_faulty_denoised &lt;- median_filter( data$y_noise_faulty, window_width=window_w ) ggplot(data, aes(x=x, y=y_noise_faulty_denoised)) + geom_point() 13.4.1 Exercises Do you notice any differences in the results of applying a median filter versus applying a rolling average? Generate a small sequence of 10 random values. On paper, apply a median filter to your sequence. You decide the window size. What happens if you increase your window size? What about if you decrease your window size? Is the median filter a good denoising approach for y_noise? What about y_noise_faulty? Try calculating the mean squared error for each of y_noise and y_noise_denoised. Does error go down? What about for y_noise_faulty and y_noise_faulty_denoised? Did the median filter improve how representative y_noise_faulty is of the ground truth signal? What happens when you make the median filter window smaller? Bigger? What happens when you add more noise to the signal? Identify any lines of code that you don’t understand. Use the documentation to figure out what those lines of code are doing. 13.5 Further exploration As we talked about in class, the appropriate-ness of different denoising techniques will depend on your data and your particular application domain. The R community has implemented many denoising techniques. For example, the signal package includes many signal denoising functions. "],["dealing-with-missing-values.html", "Chapter 14 Dealing with missing values 14.1 Dependencies and setup 14.2 Testing for missing values 14.3 Dropping columns with missing values (feature elimination) 14.4 Dropping rows with missing values (object elimination) 14.5 Mean imputation 14.6 Median imputation 14.7 Exercises", " Chapter 14 Dealing with missing values In this lab activity, we will use R to deal with missing values in a dataset. There are many methods of dealing with missing values in a dataset. Here, we’ll cover how to apply a few simple/basic approaches in R: Feature elimination (remove columns with missing values) Object elimination (remove rows with missing values) Mean imputation Median imputation This activity is meant to server as a “how to” guide and does not discuss when different methods of handling missing values should be applied. Whether or not a particular approach is appropriate will depend on your particular dataset and domain. 14.1 Dependencies and setup We’ll need to load the tidyverse packages for this lab activity: library(tidyverse) 14.2 Testing for missing values In R, missing values are often represented by NA (or some other value particular to your data; e.g., -1, none, etc). You can identify missing values using is.na() (run ?is.na to see documentation). In this demo, we’ll play with the airquality dataset, which comes loaded automatically in R. head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 There are NA values in the airquality dataset. To get a binary dataframe that indicates which positions have NA values, you can: airquality_na &lt;- is.na(airquality) head(airquality_na) ## Ozone Solar.R Wind Temp Month Day ## [1,] FALSE FALSE FALSE FALSE FALSE FALSE ## [2,] FALSE FALSE FALSE FALSE FALSE FALSE ## [3,] FALSE FALSE FALSE FALSE FALSE FALSE ## [4,] FALSE FALSE FALSE FALSE FALSE FALSE ## [5,] TRUE TRUE FALSE FALSE FALSE FALSE ## [6,] FALSE TRUE FALSE FALSE FALSE FALSE 14.3 Dropping columns with missing values (feature elimination) First, let’s get a list of columns that contain missing values. columns_with_na &lt;- c() for (col in colnames(airquality)) { if (NA %in% airquality[,col]) { columns_with_na &lt;- c(columns_with_na, col) } } print(columns_with_na) ## [1] &quot;Ozone&quot; &quot;Solar.R&quot; You can verify that against the airquality_na matrix we created earlier. To drop columns, we can use the select function in dplyr. dropped_cols_data &lt;- airquality %&gt;% select( !all_of(columns_with_na) ) head(dropped_cols_data) ## Wind Temp Month Day ## 1 7.4 67 5 1 ## 2 8.0 72 5 2 ## 3 12.6 74 5 3 ## 4 11.5 62 5 4 ## 5 14.3 56 5 5 ## 6 14.9 66 5 6 14.4 Dropping rows with missing values (object elimination) If you know exactly what columns have missing values in them, you can use filter to filter out rows where those columns have NA. dropped_rows_data &lt;- airquality %&gt;% filter( !(is.na(Ozone) | is.na(Solar.R)) ) head(dropped_rows_data) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 23 299 8.6 65 5 7 ## 6 19 99 13.8 59 5 8 A little bit more generic, we can use the matrix we generated earlier, airquality_na. # First, let&#39;s figure out all of the row ids that we want to drop drop_rows &lt;- c() for (ri in 1:nrow(airquality_na)) { if (TRUE %in% as.vector(airquality_na[ri,])) { drop_rows &lt;- c(drop_rows, ri) } } # We can use filter to drop all of the row ids in drop_rows dropped_rows_data2 &lt;- airquality %&gt;% filter( !(row_number() %in% drop_rows) ) head(dropped_rows_data2) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 23 299 8.6 65 5 7 ## 6 19 99 13.8 59 5 8 14.5 Mean imputation A mean imputation replaces missing values for a particular feature with the mean of other values for that feature. We can use dplyr functions to pretty easily perform a mean imputation on a particular column: airquality %&gt;% mutate( Ozone=ifelse(is.na(Ozone), mean(Ozone, na.rm=TRUE), Ozone) ) ## Ozone Solar.R Wind Temp Month Day ## 1 41.00000 190 7.4 67 5 1 ## 2 36.00000 118 8.0 72 5 2 ## 3 12.00000 149 12.6 74 5 3 ## 4 18.00000 313 11.5 62 5 4 ## 5 42.12931 NA 14.3 56 5 5 ## 6 28.00000 NA 14.9 66 5 6 ## 7 23.00000 299 8.6 65 5 7 ## 8 19.00000 99 13.8 59 5 8 ## 9 8.00000 19 20.1 61 5 9 ## 10 42.12931 194 8.6 69 5 10 ## 11 7.00000 NA 6.9 74 5 11 ## 12 16.00000 256 9.7 69 5 12 ## 13 11.00000 290 9.2 66 5 13 ## 14 14.00000 274 10.9 68 5 14 ## 15 18.00000 65 13.2 58 5 15 ## 16 14.00000 334 11.5 64 5 16 ## 17 34.00000 307 12.0 66 5 17 ## 18 6.00000 78 18.4 57 5 18 ## 19 30.00000 322 11.5 68 5 19 ## 20 11.00000 44 9.7 62 5 20 ## 21 1.00000 8 9.7 59 5 21 ## 22 11.00000 320 16.6 73 5 22 ## 23 4.00000 25 9.7 61 5 23 ## 24 32.00000 92 12.0 61 5 24 ## 25 42.12931 66 16.6 57 5 25 ## 26 42.12931 266 14.9 58 5 26 ## 27 42.12931 NA 8.0 57 5 27 ## 28 23.00000 13 12.0 67 5 28 ## 29 45.00000 252 14.9 81 5 29 ## 30 115.00000 223 5.7 79 5 30 ## 31 37.00000 279 7.4 76 5 31 ## 32 42.12931 286 8.6 78 6 1 ## 33 42.12931 287 9.7 74 6 2 ## 34 42.12931 242 16.1 67 6 3 ## 35 42.12931 186 9.2 84 6 4 ## 36 42.12931 220 8.6 85 6 5 ## 37 42.12931 264 14.3 79 6 6 ## 38 29.00000 127 9.7 82 6 7 ## 39 42.12931 273 6.9 87 6 8 ## 40 71.00000 291 13.8 90 6 9 ## 41 39.00000 323 11.5 87 6 10 ## 42 42.12931 259 10.9 93 6 11 ## 43 42.12931 250 9.2 92 6 12 ## 44 23.00000 148 8.0 82 6 13 ## 45 42.12931 332 13.8 80 6 14 ## 46 42.12931 322 11.5 79 6 15 ## 47 21.00000 191 14.9 77 6 16 ## 48 37.00000 284 20.7 72 6 17 ## 49 20.00000 37 9.2 65 6 18 ## 50 12.00000 120 11.5 73 6 19 ## 51 13.00000 137 10.3 76 6 20 ## 52 42.12931 150 6.3 77 6 21 ## 53 42.12931 59 1.7 76 6 22 ## 54 42.12931 91 4.6 76 6 23 ## 55 42.12931 250 6.3 76 6 24 ## 56 42.12931 135 8.0 75 6 25 ## 57 42.12931 127 8.0 78 6 26 ## 58 42.12931 47 10.3 73 6 27 ## 59 42.12931 98 11.5 80 6 28 ## 60 42.12931 31 14.9 77 6 29 ## 61 42.12931 138 8.0 83 6 30 ## 62 135.00000 269 4.1 84 7 1 ## 63 49.00000 248 9.2 85 7 2 ## 64 32.00000 236 9.2 81 7 3 ## 65 42.12931 101 10.9 84 7 4 ## 66 64.00000 175 4.6 83 7 5 ## 67 40.00000 314 10.9 83 7 6 ## 68 77.00000 276 5.1 88 7 7 ## 69 97.00000 267 6.3 92 7 8 ## 70 97.00000 272 5.7 92 7 9 ## 71 85.00000 175 7.4 89 7 10 ## 72 42.12931 139 8.6 82 7 11 ## 73 10.00000 264 14.3 73 7 12 ## 74 27.00000 175 14.9 81 7 13 ## 75 42.12931 291 14.9 91 7 14 ## 76 7.00000 48 14.3 80 7 15 ## 77 48.00000 260 6.9 81 7 16 ## 78 35.00000 274 10.3 82 7 17 ## 79 61.00000 285 6.3 84 7 18 ## 80 79.00000 187 5.1 87 7 19 ## 81 63.00000 220 11.5 85 7 20 ## 82 16.00000 7 6.9 74 7 21 ## 83 42.12931 258 9.7 81 7 22 ## 84 42.12931 295 11.5 82 7 23 ## 85 80.00000 294 8.6 86 7 24 ## 86 108.00000 223 8.0 85 7 25 ## 87 20.00000 81 8.6 82 7 26 ## 88 52.00000 82 12.0 86 7 27 ## 89 82.00000 213 7.4 88 7 28 ## 90 50.00000 275 7.4 86 7 29 ## 91 64.00000 253 7.4 83 7 30 ## 92 59.00000 254 9.2 81 7 31 ## 93 39.00000 83 6.9 81 8 1 ## 94 9.00000 24 13.8 81 8 2 ## 95 16.00000 77 7.4 82 8 3 ## 96 78.00000 NA 6.9 86 8 4 ## 97 35.00000 NA 7.4 85 8 5 ## 98 66.00000 NA 4.6 87 8 6 ## 99 122.00000 255 4.0 89 8 7 ## 100 89.00000 229 10.3 90 8 8 ## 101 110.00000 207 8.0 90 8 9 ## 102 42.12931 222 8.6 92 8 10 ## 103 42.12931 137 11.5 86 8 11 ## 104 44.00000 192 11.5 86 8 12 ## 105 28.00000 273 11.5 82 8 13 ## 106 65.00000 157 9.7 80 8 14 ## 107 42.12931 64 11.5 79 8 15 ## 108 22.00000 71 10.3 77 8 16 ## 109 59.00000 51 6.3 79 8 17 ## 110 23.00000 115 7.4 76 8 18 ## 111 31.00000 244 10.9 78 8 19 ## 112 44.00000 190 10.3 78 8 20 ## 113 21.00000 259 15.5 77 8 21 ## 114 9.00000 36 14.3 72 8 22 ## 115 42.12931 255 12.6 75 8 23 ## 116 45.00000 212 9.7 79 8 24 ## 117 168.00000 238 3.4 81 8 25 ## 118 73.00000 215 8.0 86 8 26 ## 119 42.12931 153 5.7 88 8 27 ## 120 76.00000 203 9.7 97 8 28 ## 121 118.00000 225 2.3 94 8 29 ## 122 84.00000 237 6.3 96 8 30 ## 123 85.00000 188 6.3 94 8 31 ## 124 96.00000 167 6.9 91 9 1 ## 125 78.00000 197 5.1 92 9 2 ## 126 73.00000 183 2.8 93 9 3 ## 127 91.00000 189 4.6 93 9 4 ## 128 47.00000 95 7.4 87 9 5 ## 129 32.00000 92 15.5 84 9 6 ## 130 20.00000 252 10.9 80 9 7 ## 131 23.00000 220 10.3 78 9 8 ## 132 21.00000 230 10.9 75 9 9 ## 133 24.00000 259 9.7 73 9 10 ## 134 44.00000 236 14.9 81 9 11 ## 135 21.00000 259 15.5 76 9 12 ## 136 28.00000 238 6.3 77 9 13 ## 137 9.00000 24 10.9 71 9 14 ## 138 13.00000 112 11.5 71 9 15 ## 139 46.00000 237 6.9 78 9 16 ## 140 18.00000 224 13.8 67 9 17 ## 141 13.00000 27 10.3 76 9 18 ## 142 24.00000 238 10.3 68 9 19 ## 143 16.00000 201 8.0 82 9 20 ## 144 13.00000 238 12.6 64 9 21 ## 145 23.00000 14 9.2 71 9 22 ## 146 36.00000 139 10.3 81 9 23 ## 147 7.00000 49 10.3 69 9 24 ## 148 14.00000 20 16.6 63 9 25 ## 149 30.00000 193 6.9 70 9 26 ## 150 42.12931 145 13.2 77 9 27 ## 151 14.00000 191 14.3 75 9 28 ## 152 18.00000 131 8.0 76 9 29 ## 153 20.00000 223 11.5 68 9 30 In the code above, we mutate the Ozone column. If the value is na, we replace it with the mean of the Ozone column, and otherwise, we leave it as is. 14.6 Median imputation A median imputation replaces missing values for a particular feature with the median of other (non-missing) values for that feature. We can do the exact same thing we did for a mean imputation, but swap mean out for median: airquality %&gt;% mutate( Ozone=ifelse(is.na(Ozone), median(Ozone, na.rm=TRUE), Ozone) ) ## Ozone Solar.R Wind Temp Month Day ## 1 41.0 190 7.4 67 5 1 ## 2 36.0 118 8.0 72 5 2 ## 3 12.0 149 12.6 74 5 3 ## 4 18.0 313 11.5 62 5 4 ## 5 31.5 NA 14.3 56 5 5 ## 6 28.0 NA 14.9 66 5 6 ## 7 23.0 299 8.6 65 5 7 ## 8 19.0 99 13.8 59 5 8 ## 9 8.0 19 20.1 61 5 9 ## 10 31.5 194 8.6 69 5 10 ## 11 7.0 NA 6.9 74 5 11 ## 12 16.0 256 9.7 69 5 12 ## 13 11.0 290 9.2 66 5 13 ## 14 14.0 274 10.9 68 5 14 ## 15 18.0 65 13.2 58 5 15 ## 16 14.0 334 11.5 64 5 16 ## 17 34.0 307 12.0 66 5 17 ## 18 6.0 78 18.4 57 5 18 ## 19 30.0 322 11.5 68 5 19 ## 20 11.0 44 9.7 62 5 20 ## 21 1.0 8 9.7 59 5 21 ## 22 11.0 320 16.6 73 5 22 ## 23 4.0 25 9.7 61 5 23 ## 24 32.0 92 12.0 61 5 24 ## 25 31.5 66 16.6 57 5 25 ## 26 31.5 266 14.9 58 5 26 ## 27 31.5 NA 8.0 57 5 27 ## 28 23.0 13 12.0 67 5 28 ## 29 45.0 252 14.9 81 5 29 ## 30 115.0 223 5.7 79 5 30 ## 31 37.0 279 7.4 76 5 31 ## 32 31.5 286 8.6 78 6 1 ## 33 31.5 287 9.7 74 6 2 ## 34 31.5 242 16.1 67 6 3 ## 35 31.5 186 9.2 84 6 4 ## 36 31.5 220 8.6 85 6 5 ## 37 31.5 264 14.3 79 6 6 ## 38 29.0 127 9.7 82 6 7 ## 39 31.5 273 6.9 87 6 8 ## 40 71.0 291 13.8 90 6 9 ## 41 39.0 323 11.5 87 6 10 ## 42 31.5 259 10.9 93 6 11 ## 43 31.5 250 9.2 92 6 12 ## 44 23.0 148 8.0 82 6 13 ## 45 31.5 332 13.8 80 6 14 ## 46 31.5 322 11.5 79 6 15 ## 47 21.0 191 14.9 77 6 16 ## 48 37.0 284 20.7 72 6 17 ## 49 20.0 37 9.2 65 6 18 ## 50 12.0 120 11.5 73 6 19 ## 51 13.0 137 10.3 76 6 20 ## 52 31.5 150 6.3 77 6 21 ## 53 31.5 59 1.7 76 6 22 ## 54 31.5 91 4.6 76 6 23 ## 55 31.5 250 6.3 76 6 24 ## 56 31.5 135 8.0 75 6 25 ## 57 31.5 127 8.0 78 6 26 ## 58 31.5 47 10.3 73 6 27 ## 59 31.5 98 11.5 80 6 28 ## 60 31.5 31 14.9 77 6 29 ## 61 31.5 138 8.0 83 6 30 ## 62 135.0 269 4.1 84 7 1 ## 63 49.0 248 9.2 85 7 2 ## 64 32.0 236 9.2 81 7 3 ## 65 31.5 101 10.9 84 7 4 ## 66 64.0 175 4.6 83 7 5 ## 67 40.0 314 10.9 83 7 6 ## 68 77.0 276 5.1 88 7 7 ## 69 97.0 267 6.3 92 7 8 ## 70 97.0 272 5.7 92 7 9 ## 71 85.0 175 7.4 89 7 10 ## 72 31.5 139 8.6 82 7 11 ## 73 10.0 264 14.3 73 7 12 ## 74 27.0 175 14.9 81 7 13 ## 75 31.5 291 14.9 91 7 14 ## 76 7.0 48 14.3 80 7 15 ## 77 48.0 260 6.9 81 7 16 ## 78 35.0 274 10.3 82 7 17 ## 79 61.0 285 6.3 84 7 18 ## 80 79.0 187 5.1 87 7 19 ## 81 63.0 220 11.5 85 7 20 ## 82 16.0 7 6.9 74 7 21 ## 83 31.5 258 9.7 81 7 22 ## 84 31.5 295 11.5 82 7 23 ## 85 80.0 294 8.6 86 7 24 ## 86 108.0 223 8.0 85 7 25 ## 87 20.0 81 8.6 82 7 26 ## 88 52.0 82 12.0 86 7 27 ## 89 82.0 213 7.4 88 7 28 ## 90 50.0 275 7.4 86 7 29 ## 91 64.0 253 7.4 83 7 30 ## 92 59.0 254 9.2 81 7 31 ## 93 39.0 83 6.9 81 8 1 ## 94 9.0 24 13.8 81 8 2 ## 95 16.0 77 7.4 82 8 3 ## 96 78.0 NA 6.9 86 8 4 ## 97 35.0 NA 7.4 85 8 5 ## 98 66.0 NA 4.6 87 8 6 ## 99 122.0 255 4.0 89 8 7 ## 100 89.0 229 10.3 90 8 8 ## 101 110.0 207 8.0 90 8 9 ## 102 31.5 222 8.6 92 8 10 ## 103 31.5 137 11.5 86 8 11 ## 104 44.0 192 11.5 86 8 12 ## 105 28.0 273 11.5 82 8 13 ## 106 65.0 157 9.7 80 8 14 ## 107 31.5 64 11.5 79 8 15 ## 108 22.0 71 10.3 77 8 16 ## 109 59.0 51 6.3 79 8 17 ## 110 23.0 115 7.4 76 8 18 ## 111 31.0 244 10.9 78 8 19 ## 112 44.0 190 10.3 78 8 20 ## 113 21.0 259 15.5 77 8 21 ## 114 9.0 36 14.3 72 8 22 ## 115 31.5 255 12.6 75 8 23 ## 116 45.0 212 9.7 79 8 24 ## 117 168.0 238 3.4 81 8 25 ## 118 73.0 215 8.0 86 8 26 ## 119 31.5 153 5.7 88 8 27 ## 120 76.0 203 9.7 97 8 28 ## 121 118.0 225 2.3 94 8 29 ## 122 84.0 237 6.3 96 8 30 ## 123 85.0 188 6.3 94 8 31 ## 124 96.0 167 6.9 91 9 1 ## 125 78.0 197 5.1 92 9 2 ## 126 73.0 183 2.8 93 9 3 ## 127 91.0 189 4.6 93 9 4 ## 128 47.0 95 7.4 87 9 5 ## 129 32.0 92 15.5 84 9 6 ## 130 20.0 252 10.9 80 9 7 ## 131 23.0 220 10.3 78 9 8 ## 132 21.0 230 10.9 75 9 9 ## 133 24.0 259 9.7 73 9 10 ## 134 44.0 236 14.9 81 9 11 ## 135 21.0 259 15.5 76 9 12 ## 136 28.0 238 6.3 77 9 13 ## 137 9.0 24 10.9 71 9 14 ## 138 13.0 112 11.5 71 9 15 ## 139 46.0 237 6.9 78 9 16 ## 140 18.0 224 13.8 67 9 17 ## 141 13.0 27 10.3 76 9 18 ## 142 24.0 238 10.3 68 9 19 ## 143 16.0 201 8.0 82 9 20 ## 144 13.0 238 12.6 64 9 21 ## 145 23.0 14 9.2 71 9 22 ## 146 36.0 139 10.3 81 9 23 ## 147 7.0 49 10.3 69 9 24 ## 148 14.0 20 16.6 63 9 25 ## 149 30.0 193 6.9 70 9 26 ## 150 31.5 145 13.2 77 9 27 ## 151 14.0 191 14.3 75 9 28 ## 152 18.0 131 8.0 76 9 29 ## 153 20.0 223 11.5 68 9 30 14.7 Exercises Describe a procedure for replacing an object’s missing value for a particular feature with the value from its “nearest neighbor”? What does it mean for one object (i.e., row) to be “near” another object? How could you quantify how close two objects are to one another? How would you implement this approach in R? (either from scratch or using an existing function in a library you install/load) Assume that you have a dataset where each object has two features: a continuous feature (where some of the values are missing) and a categorical feature. In this dataset, objects of the same category are more similar to each other than to objects in different categories. You want to apply a mean imputation to this dataset to replace the missing values in the continuous feature. How could you use each object’s category to improve your mean imputation? "],["sampling-objects-in-a-dataset.html", "Chapter 15 Sampling objects in a dataset 15.1 Dependencies and setup 15.2 Random sampling 15.3 Stratified random sampling", " Chapter 15 Sampling objects in a dataset In this lab activity, we will demonstrate two methods of sampling objects in a dataset: Naive random sampling Stratified random sampling Sampling is a common method of dealing with a huge number of objects/observations in a dataset. 15.1 Dependencies and setup In this activity, we’ll be using the following R packages: library(tidyverse) We’ll sample from our handy-dandy Star Wars data set: head(starwars) ## # A tibble: 6 × 14 ## name height mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex gender homew…⁵ ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Skywal… 172 77 blond fair blue 19 male mascu… Tatooi… ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… Tatooi… ## 3 R2-D2 96 32 &lt;NA&gt; white,… red 33 none mascu… Naboo ## 4 Darth Vader 202 136 none white yellow 41.9 male mascu… Tatooi… ## 5 Leia Organa 150 49 brown light brown 19 fema… femin… Aldera… ## 6 Owen Lars 178 120 brown,… light blue 52 male mascu… Tatooi… ## # … with 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, ## # starships &lt;list&gt;, and abbreviated variable names ¹​hair_color, ²​skin_color, ## # ³​eye_color, ⁴​birth_year, ⁵​homeworld 15.2 Random sampling In naive random sampling, we simply randomly sample rows from our dataset. In this example, we’ll sample without replacement: we don’t want to characters more than once in our down-sampled dataset. sample_proportion &lt;- 0.2 # Down-sample down to just this % of our full data set sample_size &lt;- ceiling(sample_proportion * nrow(starwars)) # Sample size, round up. We can use slice_sample from the dplyr package to randomly sample star wars characters. naive_sampled_data &lt;- starwars %&gt;% slice_sample( n=sample_size, replace=FALSE ) naive_sampled_data ## # A tibble: 18 × 14 ## name height mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex gender homew…⁵ ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Finis Valo… 170 NA blond fair blue 91 male mascu… Corusc… ## 2 Bib Fortuna 180 NA none pale pink NA male mascu… Ryloth ## 3 Boba Fett 183 78.2 black fair brown 31.5 male mascu… Kamino ## 4 R5-D4 97 32 &lt;NA&gt; white,… red NA none mascu… Tatooi… ## 5 Sly Moore 178 48 none pale white NA &lt;NA&gt; &lt;NA&gt; Umbara ## 6 Anakin Sky… 188 84 blond fair blue 41.9 male mascu… Tatooi… ## 7 Dooku 193 80 white fair brown 102 male mascu… Serenno ## 8 Rugor Nass 206 NA none green orange NA male mascu… Naboo ## 9 Poggle the… 183 80 none green yellow NA male mascu… Geonos… ## 10 Ratts Tyer… 79 15 none grey, … unknown NA male mascu… Aleen … ## 11 Owen Lars 178 120 brown,… light blue 52 male mascu… Tatooi… ## 12 Wilhuff Ta… 180 NA auburn… fair blue 64 male mascu… Eriadu ## 13 Chewbacca 228 112 brown unknown blue 200 male mascu… Kashyy… ## 14 Finn NA NA black dark dark NA male mascu… &lt;NA&gt; ## 15 R4-P17 96 NA none silver… red, b… NA none femin… &lt;NA&gt; ## 16 Palpatine 170 75 grey pale yellow 82 male mascu… Naboo ## 17 Padmé Amid… 165 45 brown light brown 46 fema… femin… Naboo ## 18 Adi Gallia 184 50 none dark blue NA fema… femin… Corusc… ## # … with 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, ## # starships &lt;list&gt;, and abbreviated variable names ¹​hair_color, ²​skin_color, ## # ³​eye_color, ⁴​birth_year, ⁵​homeworld Take a look at naive_sampled_data. What are some ways that our random sample might not be representative of our larger data set (starwars)? For example, we can look at the distribution of homeworlds in our original data. # Get the distribution of homeworlds in full dataset. orig_homeworlds &lt;- starwars %&gt;% group_by( homeworld ) %&gt;% summarise( n=n() ) %&gt;% mutate( freq = n / sum(n), ) %&gt;% arrange( desc(freq) ) # Get the distribution of homeworlds in the sampled dataset. sampled_homeworlds &lt;- naive_sampled_data %&gt;% group_by( homeworld ) %&gt;% summarise( n=n() ) %&gt;% mutate( freq = n / sum(n), ) %&gt;% arrange( desc(freq) ) Let’s take a look at the distribution of homeworlds from the original dataset: ggplot( orig_homeworlds, aes( x=reorder(homeworld, -freq), y=freq ) ) + geom_bar(stat=&quot;identity&quot;) + labs( x=&quot;Homeworld&quot;, y=&quot;Frequency&quot;, title=&quot;Distribution of homeworlds for original dataset&quot; ) + theme( axis.text.x=element_text(angle=45, hjust=1) ) And now the sampled dataset: ggplot( sampled_homeworlds, aes( x=reorder(homeworld, -freq), y=freq ) ) + geom_bar(stat=&quot;identity&quot;) + labs( x=&quot;Homeworld&quot;, y=&quot;Frequency&quot;, title=&quot;Distribution of homeworlds for sampled dataset&quot; ) + theme( axis.text.x=element_text(angle=45, hjust=1) ) 15.2.1 Exercises What differences do you notice between the distribution of homeworlds in the original dataset and the sampled dataset? How could you modify slice_sample to randomly sample, but instead of weighting all rows equally, you weight each row’s likelihood of being included in the sample according to the height attribute? Hint: ?slice_sample Adjust the sample proportion parameter and resample. What happens to the distribution of species as you increase or decrease the sample size? Read the R code. Identify any lines of code that you don’t understand. Use the documentation to figure out what those lines of code are doing. 15.3 Stratified random sampling Stratified sampling can be used in cases where there are predefined groupings of objects. In stratified sampling, you enforce the level of representation for each group. For example, we might want to ensure that each species among Star Wars characters is represented in our sample. There are many ways to implement this, and the particular approach you take will depend on your objective. In this example, I’ll randomly sample each species independently, guaranteeing that at least one character of each species is represented in our sample. # Grab the set of species represented in our original dataset species &lt;- levels(as.factor(starwars$species)) # Create an empty data frame that we will store our sample stratified_sampled_data &lt;- data.frame( matrix( ncol=ncol(starwars), nrow=0, dimnames=list(NULL,colnames(starwars)) ) ) for (s in species) { # First, filter down to the species species_sample &lt;- starwars %&gt;% filter( species==s ) # Calculate sample size, guarantee &gt;= 1 sample_size = max( ceiling(nrow(species_sample)*sample_proportion), 1 ) # Sample from characters of the current species species_sample &lt;- species_sample %&gt;% slice_sample( n=sample_size, replace=FALSE ) # Add sample of this species to larger sample stratified_sampled_data &lt;- rbind( stratified_sampled_data, species_sample ) } stratified_sampled_data ## # A tibble: 44 × 14 ## name height mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex gender homew…⁵ ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Ratts Tyer… 79 15 none grey, … unknown NA male mascu… Aleen … ## 2 Dexter Jet… 198 102 none brown yellow NA male mascu… Ojom ## 3 Ki-Adi-Mun… 198 82 white pale yellow 92 male mascu… Cerea ## 4 Mas Amedda 196 NA none blue blue NA male mascu… Champa… ## 5 Zam Wesell 168 55 blonde fair, … yellow NA fema… femin… Zolan ## 6 R4-P17 96 NA none silver… red, b… NA none femin… &lt;NA&gt; ## 7 R5-D4 97 32 &lt;NA&gt; white,… red NA none mascu… Tatooi… ## 8 Sebulba 112 40 none grey, … orange NA male mascu… Malast… ## 9 Wicket Sys… 88 20 brown brown brown 8 male mascu… Endor ## 10 Poggle the… 183 80 none green yellow NA male mascu… Geonos… ## # … with 34 more rows, 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt;, and abbreviated variable names ## # ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld 15.3.1 Exercises Graph the distribution of species in the original starwars dataset and in the stratified sample (just like we did for homeworlds in the naive random sample). What differences do you notice? What might be causing those differences? We specified a sample proportion of 0.2. Why might the size of our stratified sample differ from the size of our random sample? Read the R code. Identify any lines of code that you don’t understand. Use the documentation to figure out what those lines of code are doing. "],["curse-of-dimensionality-demo.html", "Chapter 16 Curse of dimensionality demo 16.1 Dependencies 16.2 Generate example data 16.3 Exercises", " Chapter 16 Curse of dimensionality demo As the dimensionality of your data increases, your data become increasingly sparse in the space that they occupy. As a result, the distances between objects in your dataset become less meaningful as dimensionality increases. This dynamic can make some data mining techniques less effective (e.g., clustering or outlier detection). In this activity, we will demonstrate the effect of dimensionality on distances in data. 16.1 Dependencies We’ll use the rdist package to calculate distances between points. Look over the documentation for rdist here: https://github.com/blasern/rdist. The code chunk below will check if you have the rdist package. If not, it will install it. rdist_available &lt;- require(rdist) ## Loading required package: rdist if (!rdist_available) { install.packages(&quot;rdist&quot;) } library(rdist) We’ll also use the following libraries (if you don’t have these, you’ll need to install them): library(tidyverse) 16.2 Generate example data num_points &lt;- 500 min_range &lt;- 0 max_range &lt;- 1 dimension_sizes &lt;- 2:50 dist_metrics &lt;- c(&quot;euclidean&quot;, &quot;angular&quot;) Generate 500 points in an N-dimensional box (where each dimension of the box ranges from 0 and 1). data &lt;- data.frame(num_dimensions=c()) # For each dimensionality, randomly generate &quot;num_points&quot; points in our rectangle. for (num_dimensions in dimension_sizes) { # Generate random points in our N-dimensional box. points_matrix &lt;- matrix( data = runif(num_points * num_dimensions), nrow = num_points, ncol = num_dimensions ) # Calculate the pairwise distances between points in our dataset. # Run ?rdist to see how the rdist function works for (cur_metric in dist_metrics) { distances &lt;- c(rdist(points_matrix, metric=cur_metric)) dim_data &lt;- data.frame( max_dist = c(max(distances)), min_dist = c(min(distances)), metric = c(cur_metric), num_dimensions = num_dimensions ) data &lt;- rbind(data, dim_data) } } We can get a rough idea of how meaningful the distances in our space are by calculating (max_dist-min_dist)/min_dist. ggplot( data, aes( x=num_dimensions, y=((max_dist-min_dist)/min_dist), color=metric, fill=metric ) ) + geom_point() + scale_y_continuous( name=&quot;log10((max_dist-min_dist)/min_dist)&quot;, trans=&quot;log10&quot; ) + labs( x=&quot;Number of dimensions&quot;, color=&quot;Metric&quot;, fill=&quot;Metric&quot; ) 16.3 Exercises Think about the meaning of (max_dist-min_dist)/min_dist. What’s going on as we increase dimensionality? Read the R code in this document. Identify any lines of code that you don’t understand. Use the documentation to figure out what those lines of code are doing. "],["aggregation.html", "Chapter 17 Aggregation 17.1 Dependencies and setup 17.2 Exercise 17.3 Using dplyr to aggregate objects in a dataset according to a category 17.4 Exercises", " Chapter 17 Aggregation Sometimes we want to combine multiple objects in a dataset into a single object. For example, we often want to combine objects according to some categorical variable. We are often interested in the number of objects that go into each aggregate as well as various summary statistics; e.g., mean median mode standard deviation variance minimum value maximum value range If you are not familiar any of those summary statistics, look them up! 17.1 Dependencies and setup We’ll be using the dplyr package, which comes loaded in the tidyverse collection of packages. library(tidyverse) 17.2 Exercise Given the following vector, y y &lt;- runif(100, min=-1000, max=1000) Write R code to calculate each of the following: mean median mode standard deviation variance minimum value maximum value range 17.3 Using dplyr to aggregate objects in a dataset according to a category In many cases, you will be able to use a combination of the group_by and summarise functions (from dplyr) to aggregate categories of data. data &lt;- starwars data ## # A tibble: 87 × 14 ## name height mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex gender homew…⁵ ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Skywa… 172 77 blond fair blue 19 male mascu… Tatooi… ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… Tatooi… ## 3 R2-D2 96 32 &lt;NA&gt; white,… red 33 none mascu… Naboo ## 4 Darth Vader 202 136 none white yellow 41.9 male mascu… Tatooi… ## 5 Leia Organa 150 49 brown light brown 19 fema… femin… Aldera… ## 6 Owen Lars 178 120 brown,… light blue 52 male mascu… Tatooi… ## 7 Beru White… 165 75 brown light blue 47 fema… femin… Tatooi… ## 8 R5-D4 97 32 &lt;NA&gt; white,… red NA none mascu… Tatooi… ## 9 Biggs Dark… 183 84 black light brown 24 male mascu… Tatooi… ## 10 Obi-Wan Ke… 182 77 auburn… fair blue-g… 57 male mascu… Stewjon ## # … with 77 more rows, 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt;, and abbreviated variable names ## # ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld Let’s aggregate characters in the Star Wars dataset according to their species. In this particular example, we’ll focus on summarize the mass values within each species. # First let&#39;s filter out all characters with NA for mass or species. data$species &lt;- as.factor(data$species) data &lt;- data %&gt;% filter( !(is.na(species) | is.na(mass)) ) agg_data &lt;- data %&gt;% group_by( species ) %&gt;% summarise( mass_median=median(mass), mass_mean=mean(mass), mass_sd=sd(mass), mass_min=min(mass), mass_max=max(mass), mass_total=sum(mass), num_characters=n() ) print(agg_data) ## # A tibble: 31 × 8 ## species mass_median mass_mean mass_sd mass_min mass_max mass_total num_ch…¹ ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Aleena 15 15 NA 15 15 15 1 ## 2 Besalisk 102 102 NA 102 102 102 1 ## 3 Cerean 82 82 NA 82 82 82 1 ## 4 Clawdite 55 55 NA 55 55 55 1 ## 5 Droid 53.5 69.8 51.0 32 140 279 4 ## 6 Dug 40 40 NA 40 40 40 1 ## 7 Ewok 20 20 NA 20 20 20 1 ## 8 Geonosian 80 80 NA 80 80 80 1 ## 9 Gungan 74 74 11.3 66 82 148 2 ## 10 Human 79 82.8 19.4 45 136 1821. 22 ## # … with 21 more rows, and abbreviated variable name ¹​num_characters 17.4 Exercises Try loading in some data that you have used in a previous homework assignment (or any dataset of your choice), and combine objects in your dataset using a combination of group_by and summarize. Identify any lines of code that you don’t understand. Use the documentation to figure out what those lines of code are doing. "],["building-a-decision-tree-in-r.html", "Chapter 18 Building a decision tree in R 18.1 Dependencies and setup 18.2 Building a decision tree 18.3 Visualizing our decision tree 18.4 Evaluating the accuracy of our decision tree 18.5 Using our decision tree to make a prediction 18.6 Exercises", " Chapter 18 Building a decision tree in R In this lab activity, we will use the rpart R package to build a decision tree classifier. 18.1 Dependencies and setup We’ll use the rpart, rpart.plot, and caret packages in this activity. If you do not have these packages installed, you will need to do so (use the install.packages command). library(rpart) # Used to build decision tree model library(rpart.plot) # Used to visualize trees library(caret) # Used to build a fancy confusion matrix ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## lift I encourage you to look up the documentation for these packages. We’ll also use the following packages: library(tidyverse) Next, we’ll load the dataset that contains data for the 100 US senators as of summer 2020. Depending on your working directory, you may need to adjust the path to the data file. senator_data &lt;- read_csv(&quot;lecture-material/week-06/senate-data.csv&quot;) ## Rows: 100 Columns: 14 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): first, last, pol ## dbl (11): age, law, mil, soc, edu, med, bus, agr, fin, years, approval ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. senator_data$pol &lt;- as.factor(senator_data$pol) senator_data$law &lt;- as.factor(senator_data$law) senator_data$mil &lt;- as.factor(senator_data$mil) senator_data$soc &lt;- as.factor(senator_data$soc) senator_data$edu &lt;- as.factor(senator_data$edu) senator_data$med &lt;- as.factor(senator_data$med) senator_data$bus &lt;- as.factor(senator_data$bus) senator_data$agr &lt;- as.factor(senator_data$agr) senator_data$fin &lt;- as.factor(senator_data$fin) senator_data$approval &lt;- as.factor(senator_data$approval) head(senator_data) ## # A tibble: 6 × 14 ## first last pol age law mil soc edu med bus agr fin years ## &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Richa… Shel… R 86 1 0 0 0 0 0 0 0 33 ## 2 Doug Jones D 66 1 0 0 0 0 0 0 0 2 ## 3 Lisa Murk… R 63 1 0 0 0 0 0 0 0 17 ## 4 Dan Sull… R 55 1 1 0 0 0 0 0 0 5 ## 5 Kyrst… Sine… D 43 1 0 1 0 1 0 0 0 1 ## 6 Martha McSa… R 54 0 1 0 0 0 0 0 0 1 ## # … with 1 more variable: approval &lt;fct&gt; These data were taken from a website of senator rankings. Each row in these data describes a senator with the following attributes: first - First name last - Last name pol - Political party (democrat, republican, or independent) age - Age of the senator law - Binary attribute describing whether senator had a previous career in law enforcement mil - Binary attribute describing whether senator had a previous career in the military soc - Binary attribute describing whether senator had a previous career in social services edu - Binary attribute describing whether senator had a previous career in education med - Binary attribute describing whether senator had a previous career in medicine bus - Binary attribute describing whether senator had a previous career in business agr - Binary attribute describing whether senator had a previous career in aggriculture fin - Binary attribute describing whether senator had a previous career in finance years - Number of years in office approval - Approval class. class=0 means voters don’t approve of them, and class=1 means that voters do approve of them. 18.2 Building a decision tree We can use the rpart function to build a decison tree: model &lt;- rpart( formula = approval ~ pol + age + law + mil + soc + edu + med + bus + agr + fin + years, data = senator_data, parms = list(split=&quot;information&quot;) ) model ## n= 100 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 100 41 1 (0.4100000 0.5900000) ## 2) age&lt; 66.5 59 29 1 (0.4915254 0.5084746) ## 4) years&lt; 2.5 10 2 0 (0.8000000 0.2000000) * ## 5) years&gt;=2.5 49 21 1 (0.4285714 0.5714286) ## 10) pol=R 26 13 0 (0.5000000 0.5000000) ## 20) age&gt;=50.5 18 8 0 (0.5555556 0.4444444) * ## 21) age&lt; 50.5 8 3 1 (0.3750000 0.6250000) * ## 11) pol=D 23 8 1 (0.3478261 0.6521739) * ## 3) age&gt;=66.5 41 12 1 (0.2926829 0.7073171) * If you’ve never seen the formula syntax before, I recommend reading up on it a bit: https://r4ds.had.co.nz/model-basics.html?q=formula#formulas-and-model-families In the formula used to build our decision tree model, approval is our dependent variable and all of the predictor attributes to the right of the ~ are our independent variables. We can use the summary function for a little bit more information about our model: summary(model) ## Call: ## rpart(formula = approval ~ pol + age + law + mil + soc + edu + ## med + bus + agr + fin + years, data = senator_data, parms = list(split = &quot;information&quot;)) ## n= 100 ## ## CP nsplit rel error xerror xstd ## 1 0.07317073 0 1.0000000 1.000000 0.1199593 ## 2 0.02439024 2 0.8536585 1.414634 0.1203801 ## 3 0.01000000 4 0.8048780 1.414634 0.1203801 ## ## Variable importance ## years age pol agr law edu mil ## 46 36 9 5 2 1 1 ## ## Node number 1: 100 observations, complexity param=0.07317073 ## predicted class=1 expected loss=0.41 P(node) =1 ## class counts: 41 59 ## probabilities: 0.410 0.590 ## left son=2 (59 obs) right son=3 (41 obs) ## Primary splits: ## age &lt; 66.5 to the left, improve=2.0126500, (0 missing) ## years &lt; 2.5 to the left, improve=1.2801170, (0 missing) ## pol splits as RRL, improve=0.8920908, (0 missing) ## fin splits as RL, improve=0.3974025, (0 missing) ## soc splits as RL, improve=0.1539107, (0 missing) ## Surrogate splits: ## years &lt; 16 to the left, agree=0.75, adj=0.390, (0 split) ## pol splits as LRL, agree=0.61, adj=0.049, (0 split) ## agr splits as LR, agree=0.61, adj=0.049, (0 split) ## mil splits as LR, agree=0.60, adj=0.024, (0 split) ## edu splits as LR, agree=0.60, adj=0.024, (0 split) ## ## Node number 2: 59 observations, complexity param=0.07317073 ## predicted class=1 expected loss=0.4915254 P(node) =0.59 ## class counts: 29 30 ## probabilities: 0.492 0.508 ## left son=4 (10 obs) right son=5 (49 obs) ## Primary splits: ## years &lt; 2.5 to the left, improve=2.4206870, (0 missing) ## bus splits as RL, improve=0.7882244, (0 missing) ## age &lt; 58.5 to the left, improve=0.4144379, (0 missing) ## pol splits as R-L, improve=0.2210771, (0 missing) ## law splits as RL, improve=0.2084032, (0 missing) ## Surrogate splits: ## age &lt; 44 to the left, agree=0.847, adj=0.1, (0 split) ## agr splits as RL, agree=0.847, adj=0.1, (0 split) ## ## Node number 3: 41 observations ## predicted class=1 expected loss=0.2926829 P(node) =0.41 ## class counts: 12 29 ## probabilities: 0.293 0.707 ## ## Node number 4: 10 observations ## predicted class=0 expected loss=0.2 P(node) =0.1 ## class counts: 8 2 ## probabilities: 0.800 0.200 ## ## Node number 5: 49 observations, complexity param=0.02439024 ## predicted class=1 expected loss=0.4285714 P(node) =0.49 ## class counts: 21 28 ## probabilities: 0.429 0.571 ## left son=10 (26 obs) right son=11 (23 obs) ## Primary splits: ## pol splits as R-L, improve=0.5805888, (0 missing) ## age &lt; 60.5 to the left, improve=0.4295409, (0 missing) ## law splits as RL, improve=0.2763052, (0 missing) ## years &lt; 8 to the right, improve=0.2186185, (0 missing) ## bus splits as RL, improve=0.1644552, (0 missing) ## Surrogate splits: ## years &lt; 10 to the left, agree=0.653, adj=0.261, (0 split) ## law splits as LR, agree=0.633, adj=0.217, (0 split) ## age &lt; 50 to the left, agree=0.571, adj=0.087, (0 split) ## soc splits as LR, agree=0.551, adj=0.043, (0 split) ## bus splits as RL, agree=0.551, adj=0.043, (0 split) ## ## Node number 10: 26 observations, complexity param=0.02439024 ## predicted class=0 expected loss=0.5 P(node) =0.26 ## class counts: 13 13 ## probabilities: 0.500 0.500 ## left son=20 (18 obs) right son=21 (8 obs) ## Primary splits: ## age &lt; 50.5 to the right, improve=0.3640124, (0 missing) ## years &lt; 8 to the right, improve=0.3107940, (0 missing) ## ## Node number 11: 23 observations ## predicted class=1 expected loss=0.3478261 P(node) =0.23 ## class counts: 8 15 ## probabilities: 0.348 0.652 ## ## Node number 20: 18 observations ## predicted class=0 expected loss=0.4444444 P(node) =0.18 ## class counts: 10 8 ## probabilities: 0.556 0.444 ## ## Node number 21: 8 observations ## predicted class=1 expected loss=0.375 P(node) =0.08 ## class counts: 3 5 ## probabilities: 0.375 0.625 18.3 Visualizing our decision tree We’ll use functionality from the rpart.plot package to visualize the decision tree we built: # If you want to save the visualization, uncomment the next few lines pdf(&quot;decision_tree.pdf&quot;) rpart.plot(model) dev.off() ## quartz_off_screen ## 2 # (this isn&#39;t using ggplot, so we can&#39;t use ggsave) # Redraw plot to make it show up in our Rmd doc rpart.plot(model) 18.4 Evaluating the accuracy of our decision tree Our goal with this model was primarily descriptive. That is, we wanted to learn more about the properties of our senators dataset instead of building a model to predict the approval rating of unknown senators. As such, we did not create an explicit training and testing set. In essence, the entire senator dataset is our “training set”. So, while we don’t have a testing set to evaluate our model, we can evaluate our model’s accuracy on the training set. ground_truth &lt;- senator_data$approval training_predictions &lt;- predict( model, data=senator_data, type=&quot;class&quot; ) confusion_matrix &lt;- table(ground_truth, training_predictions) confusion_matrix ## training_predictions ## ground_truth 0 1 ## 0 18 23 ## 1 10 49 We can also use the confusionMatrix function from the caret package to build a fancier confusion matrix for us. confusionMatrix(confusion_matrix) ## Confusion Matrix and Statistics ## ## training_predictions ## ground_truth 0 1 ## 0 18 23 ## 1 10 49 ## ## Accuracy : 0.67 ## 95% CI : (0.5688, 0.7608) ## No Information Rate : 0.72 ## P-Value [Acc &gt; NIR] : 0.88840 ## ## Kappa : 0.2832 ## ## Mcnemar&#39;s Test P-Value : 0.03671 ## ## Sensitivity : 0.6429 ## Specificity : 0.6806 ## Pos Pred Value : 0.4390 ## Neg Pred Value : 0.8305 ## Prevalence : 0.2800 ## Detection Rate : 0.1800 ## Detection Prevalence : 0.4100 ## Balanced Accuracy : 0.6617 ## ## &#39;Positive&#39; Class : 0 ## 18.5 Using our decision tree to make a prediction Let’s pretend that a mysterious new senator shows up to join the US senate one day (ignoring any and all laws that would prevent this from happening in reality). # Make an empty dataframe w/correct attributes to hold new senator. new_senator &lt;- senator_data[0,0:13] # Bind new mystery senator to empty dataframe new_senator &lt;- rbind( new_senator, list( fist=&quot;Mystery&quot;, last=&quot;Senator&quot;, pol=&quot;I&quot;, age=42, law=0, mil=0, soc=0, edu=0, med=0, bus=0, agr=0, fin=0, years=0 ) ) # Turn new senator columsn into factors where appropriate new_senator$pol &lt;- as.factor(new_senator$pol) new_senator$law &lt;- as.factor(new_senator$law) new_senator$mil &lt;- as.factor(new_senator$mil) new_senator$soc &lt;- as.factor(new_senator$soc) new_senator$edu &lt;- as.factor(new_senator$edu) new_senator$med &lt;- as.factor(new_senator$med) new_senator$bus &lt;- as.factor(new_senator$bus) new_senator$agr &lt;- as.factor(new_senator$agr) new_senator$fin &lt;- as.factor(new_senator$fin) # I know that was all a little annoying just to predict a hand-crafted example. # Usually the things you want to predict are in a file, so it&#39;s less annoying to # load them in from file... result &lt;- predict( model, newdata=new_senator, type=&quot;class&quot; ) result[[1]] ## [1] 0 ## Levels: 0 1 It looks like we would predict that Mystery Senator would not have voter approval (according to our model). 18.6 Exercises Identify any lines of code that you do not understand. Use the documentation to figure out what is going on. Describe the decision tree. What is the most informative attribute for determining whether or not a senator has voter approval? How would you describe the performance of our model? What is its training error? Try adjusting the attributes for the Mystery Senator we created. What attributes can you change to change our model’s prediction? Identify a dataset that might be interesting to build a decision tree model for. Load that dataset into R Partition the data into a training and testing set (i.e., split the rows) Use rpart to build a decision tree using the training set. What is the accuracy of your model on the training set? What is the accuracy of your model on the testing set? "],["homework-4.html", "Chapter 19 Homework 4 19.1 Overview 19.2 Part A. Identify relevant publications 19.3 Part B.", " Chapter 19 Homework 4 19.1 Overview Being able to search for and read academic publications is useful for learning about data mining techniques (and for doing your final projects). In this assignment, you will find and describe academic publications related to classification. 19.1.1 Objectives Practice using academic search engines Gain experience reading academic papers Describe, at a high level, the findings presented in an academic paper 19.1.2 Grading Uploaded requested files, 5% File is properly formatted, 5% Writing is clear, 5% Part A, 45% For each of the three papers: 1: 5% 2: 5% 3: 5% Part B, 40% 1: 10% 2: 20% 3: 10% 19.1.3 Deliverables A .pdf file with your responses, formatted as in my example. 19.1.4 Formatting Include headings for each part (Part A and Part B). Clearly indicate which question each of your responses belong to, and for Part A, clearly indicate each paper (paper 1, paper 2, and paper 3). See the example formatting that is posted on blackboard as an attachment to this assignment. You do not need to use R markdown to generate your pdf for this assignment. However, your formatting should mirror the format of my example file. 19.2 Part A. Identify relevant publications For Part A, use an academic search engine (e.g., google scholar) to find three publications related to classification that you think might be interesting. Each of the chosen papers should be a scholarly publication; they should not be, for example, news articles, promotional materials, or blog posts. For each of the three papers you find, provide the following: Citation information (at minimum include authors, year published, title, and journal/conference/book published in) Note that I must be able to find your paper using your citation information Abstract How did you find the paper? 19.3 Part B. For Part B, choose one of the three papers you found for Part A. Read the paper to the best of your ability. It’s okay if you don’t fully understand everything in the paper. Answer the following questions. Your responses should be in complete sentences. None of your responses need to be more than a paragraph, but it should be clear that you have read and made an effort to understand the chosen paper. Why did you choose to read this paper? In your own words (1 - 2 paragraphs), what is the paper about? E.g., what are the major results, the problem being solved, techniques used, etc. To the best of your understanding, what classification algorithms are discussed and/or used in the paper? "],["linear-regression-in-r.html", "Chapter 20 Linear regression in R 20.1 Dependencies and setup 20.2 Fitting a linear model to data 20.3 Exercises", " Chapter 20 Linear regression in R This lab activity is adapted from the R for Data Science book. In this lab activity, we step through the rationale for linear regression. 20.1 Dependencies and setup This lab using the following packages: - tidyverse - modelr - cowplot You will need to install any that you do not have. library(tidyverse) library(modelr) # The cowplot package has a nice theme for ggplot *and* all sorts of useful # other features. library(cowplot) # With theme_set, we can set the default ggplot theme to be the cowplot theme. theme_set(theme_cowplot()) 20.2 Fitting a linear model to data sim1 is a simple dataset included in the modelr package. sim1 has two attributes: y and x. In this lab activity, sim1 is our training data wherein y is our response variable and x is a predictor variable. That is, we want to use linear regression to build a model that describes the relationship between our response variable, y, and our predictor variable, x. We could then use our model to make predictions about values of y for unseen values of x. head(sim1) ## # A tibble: 6 × 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 4.20 ## 2 1 7.51 ## 3 1 2.13 ## 4 2 8.99 ## 5 2 10.2 ## 6 2 11.3 Let’s plot our training data: s1_plot &lt;- ggplot(sim1, aes(x, y)) + geom_point(size=3) + theme( text=element_text(size=16) ) s1_plot 20.2.1 Random regression models In a simple linear regression, we assume a linear relationship between our response variable y and our predictor variable x. We can describe this relationship mathematically as follows: \\[y=a_2*x + a_1\\] Notice how we set up that function such that y depends on three things: the value of our predictor variable x, \\(a_2\\), and \\(a_1\\). \\(a_2\\) and $a_1 parameterize our linear model. That is, the values of \\(a_2\\) and \\(a_1\\) specify the particular linear relationship between x and y. \\(a_2\\) gives the slope of the line and \\(a_1\\) gives the y-intercept for the line. Every combination of values for \\(a_2\\) and \\(a_1\\) is a different model. For example, we could generate 500 different models for our sim1 data by generating 500 random pairs of values for \\(a_1\\) and \\(a_2\\): models &lt;- tibble( a1 = runif(500, -20, 40), a2 = runif(500, -5, 5) ) head(models) ## # A tibble: 6 × 2 ## a1 a2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 10.9 -1.25 ## 2 -0.890 -4.75 ## 3 -19.7 -4.30 ## 4 -10.4 -0.882 ## 5 -18.3 -2.44 ## 6 23.6 -1.51 Let’s plot all 500 of those random models on top of our sim1 data: rand_search_plot &lt;- ggplot(sim1, aes(x, y)) + geom_abline( data = models, aes(intercept = a1, slope = a2), alpha = 1/6 ) + geom_point( size=3 ) + theme( text = element_text(size=16) ) rand_search_plot Each light gray line in the plot above is a different possible model for our sim1 data. However, not all models are an equally good fit for our data – we generated them all randomly, so we shouldn’t have any expectation for many of them to be good! So, we need a way to assess the quality of each model. The function below will run our linear model with a given parameterization for \\(a_1\\) and \\(a_2\\). # run_model &lt;- function(a, data) { # a[1] + data$x * a[2] # } # model1(c(7, 1.5), sim1) run_model &lt;- function(a1, a2, data) { return(a1 + (data$x * a2) ) } For example, if we wanted to run our linear model using the x values from our training data (sim1) with \\(a_1=7\\) and \\(a_2=1.5\\): run_model(7, 1.5, sim1) ## [1] 8.5 8.5 8.5 10.0 10.0 10.0 11.5 11.5 11.5 13.0 13.0 13.0 14.5 14.5 14.5 ## [16] 16.0 16.0 16.0 17.5 17.5 17.5 19.0 19.0 19.0 20.5 20.5 20.5 22.0 22.0 22.0 That output gives the predicted y value for each x value in our training set if we were to parameterize our model with \\(a_1=7\\) and \\(a_2=1.5\\) Next, we need a way to measure the error between the value of our response variable in our training data and the prediction made by our model. # measure_error returns the sum of squared error between the model output # and the true value of the response variable in our training data. measure_error &lt;- function(model_params, data) { diff &lt;- data$y - run_model(model_params[1], model_params[2], data) return(sum(diff ^ 2)) } For example, we can use the above measure_error function to calculate the sum of squared error for the linear model defined by \\(a_1=7\\) and \\(a_2=1.5\\) measure_error(c(7, 1.5), sim1) ## [1] 213.1007 We can now measure the error for all 500 of our randomly generated models: # I define this sim1_err function to make it easy to use mapply to compute # the error for *all* models at once. sim1_err &lt;- function(a1, a2) { measure_error(c(a1, a2), sim1) } models$error &lt;- mapply( sim1_err, models$a1, models$a2 ) models ## # A tibble: 500 × 3 ## a1 a2 error ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10.9 -1.25 6792. ## 2 -0.890 -4.75 65789. ## 3 -19.7 -4.30 114090. ## 4 -10.4 -0.882 30707. ## 5 -18.3 -2.44 72051. ## 6 23.6 -1.51 3281. ## 7 -19.7 1.61 21000. ## 8 38.2 1.46 28517. ## 9 1.89 -2.15 23968. ## 10 37.3 -0.156 14517. ## # … with 490 more rows Notice that models describes 500 models (i.e., defines 500 pairs of values for a1 and a2), and now gives the error for each model. With those error values, we can start to look at the best of our 500 models. Let’s look at the best 3 models out of the 500 random models we generated: top_models_plot &lt;- ggplot( sim1, aes(x, y) ) + geom_point( size = 2, colour = &quot;grey30&quot; ) + geom_abline( aes(intercept = a1, slope = a2, colour = -error), data = filter(models, rank(error) &lt;= 3) ) + scale_color_continuous( name = &quot;Error&quot; ) + theme( text = element_text(size = 16) ) top_models_plot 20.2.1.1 Model space We could think of each model (defined by a1 and a2) as a point in “model space” (i.e., the space of all possible model parameterizations). That is, we could plot all 500 models we randomly generated according to their value for a1 and a2. ggplot(models, aes(a1, a2)) + geom_point( aes(colour = -error) ) + scale_color_continuous( name=&quot;Error&quot; ) + theme( text=element_text(size=16) ) Now, let’s highlight (in red) the three best models we found in model space: ggplot(models, aes(a1, a2)) + geom_point( data = filter(models, rank(error) &lt;= 3), size = 4, colour = &quot;red&quot; ) + geom_point( aes(colour = -error) ) + scale_color_continuous( name=&quot;Error&quot; ) + theme( text=element_text(size=16) ) 20.2.2 More systematically searching model space for good models Instead of generating 500 random models, we could generate 500 models that are evenly spaced within some portion of “model space”. grid &lt;- expand.grid( a1 = seq(-5, 20, length = 50), a2 = seq(1, 3, length = 50) ) head(grid) ## a1 a2 ## 1 -5.000000 1 ## 2 -4.489796 1 ## 3 -3.979592 1 ## 4 -3.469388 1 ## 5 -2.959184 1 ## 6 -2.448980 1 grid has 500 evenly spaced models: ggplot( data=grid, aes(a1, a2) ) + geom_point() + theme( text=element_text(size=16) ) We can calculate the error for each model: grid &lt;- grid %&gt;% mutate(error = purrr::map2_dbl(a1, a2, sim1_err)) head(grid) ## a1 a2 error ## 1 -5.000000 1 7163.370 ## 2 -4.489796 1 6711.865 ## 3 -3.979592 1 6275.979 ## 4 -3.469388 1 5855.711 ## 5 -2.959184 1 5451.062 ## 6 -2.448980 1 5062.031 And then plot it, highlighting the top 3 models: ggplot( data=grid, aes(a1, a2) ) + geom_point( data = filter(grid, rank(error) &lt;= 3), size = 4, colour = &quot;red&quot; ) + geom_point( aes(colour = -error) ) + scale_color_continuous( name=&quot;Error&quot; ) + theme( text=element_text(size=16) ) What do those top 3 models look like in terms of our predictor and response variable? ggplot( sim1, aes(x, y) ) + geom_point( size = 2, colour = &quot;grey30&quot; ) + geom_abline( aes(intercept = a1, slope = a2, colour = -error), data = filter(grid, rank(error) &lt;= 3) ) + scale_color_continuous( name=&quot;Error&quot; ) + theme( text=element_text(size=16) ) 20.2.3 Using lm for linear regression in R Fortunately for us, we don’t need to generate random models or do a grid search in order to find good parameterizations for a linear regression model. There are optimization techniques/algorithms that will quickly find good values for a1 and a2. In R, you can use the lm function to fit linear models. The lm function using R’s formula syntax, which you will want to familiarize yourself with: https://r4ds.had.co.nz/model-basics.html#model-basics and https://r4ds.had.co.nz/model-basics.html#formulas-and-model-families give good introductions to the formula syntax in R. For example, to fit the linear model we were using to model the sim1 training data, we could: lm_mod &lt;- lm(y ~ x, data = sim1) lm_mod ## ## Call: ## lm(formula = y ~ x, data = sim1) ## ## Coefficients: ## (Intercept) x ## 4.221 2.052 20.3 Exercises Read through the R code. Identify any lines you don’t understand and figure out how they work using the R documentation. In the example above, we generated 500 random models. How would it have affected our results if we had generated just 10 random models? What about 10,000 random models? Look back to where we visualized our 500 random models in model space. What would it mean for the top 10 models to be in very different parts of model space (e.g., some in the top right and some in the bottom left)? What kind of data might that happen for? Look up multivariate linear regression. Then, look up how you could do multivariate linear regression using the lm function in R? Pick another regression approach that I listed in the overview of regression lecture. Look up what situations that regression approach is useful for, and then look up how you could do it in R (e.g., using base R functionality or with a third-party package). "],["using-a-testing-set-to-evaluate-the-quality-of-a-model.html", "Chapter 21 Using a testing set to evaluate the quality of a model 21.1 Dependencies and setup 21.2 Loading and preprocessing the data 21.3 Creating training and testing sets 21.4 Training a simple linear model 21.5 Comparing two models 21.6 Exercises 21.7 References", " Chapter 21 Using a testing set to evaluate the quality of a model In this lab activity, we will do the following: - load a dataset - randomly divide the dataset into training and testing data - train a linear model and a regression tree using the training set - compute training error - compute testing error 21.1 Dependencies and setup We’ll use the following packages in this lab activity: library(tidyverse) library(modelr) library(rpart) # Used to build a regression tree library(rpart.plot) # Used to visualize trees You’ll need to install any packages that you don’t already have installed. 21.2 Loading and preprocessing the data In this lab activity, we’ll be using the Seoul bike dataset from the UCI machine learning repository. This dataset contains the number of public bikes rented at each hour in a Seoul bike sharing system. In addition to the number of bikes, the data contains other variables, including weather and holiday information. Our goal will be to build some simple models that predict the number of bikes rented as a function of other attributes available in the dataset. # Load data from file (you will need to adjust the file path to run locally) data &lt;- read_csv(&quot;lecture-material/week-08/data/SeoulBikeData.csv&quot;) ## Rows: 8760 Columns: 14 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): date, seasons, holiday, functioning_day ## dbl (10): rented_bike_count, hour, temperature, humidity, wind_speed, visibi... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. We can look at the attributes of our data: colnames(data) ## [1] &quot;date&quot; &quot;rented_bike_count&quot; &quot;hour&quot; ## [4] &quot;temperature&quot; &quot;humidity&quot; &quot;wind_speed&quot; ## [7] &quot;visibility&quot; &quot;dew_point_temperature&quot; &quot;solar_radiation&quot; ## [10] &quot;rainfall&quot; &quot;snowfall&quot; &quot;seasons&quot; ## [13] &quot;holiday&quot; &quot;functioning_day&quot; Next, we want to convert categorical attributes into proper factors. # Convert categorical variables into factors: data$holiday &lt;- as.factor(data$holiday) data$functioning_day &lt;- as.factor(data$functioning_day) data$seasons &lt;- as.factor(data$seasons) 21.3 Creating training and testing sets Next, we want to split our full data set into a training set and a testing set. We’ll use the training set to train/build our models, and then we can use the testing set to evaluate the performance of our model on data unseen during training. # First assign ID to each row to help us make the split. data &lt;- data %&gt;% mutate(id = row_number()) # Size (as a proportion) of our training set. training_set_size &lt;- 0.5 # Use slice sample create a training set comprising a sample of the rows from # the full dataset. training_data &lt;- data %&gt;% slice_sample(prop = training_set_size) # The testing set should be all of the rows in data not in the training set # For this, we can use the &#39;anti_join&#39; dplyr function testing_data &lt;- data %&gt;% anti_join(training_data, by = &quot;id&quot;) # Alternatively, we could have used the filter function to do the same thing # testing_data &lt;- data %&gt;% # filter(!(id %in% training_data$id)) 21.4 Training a simple linear model Next, we’ll use linear regression to estimate a simple model of rented_bike_count as a function of the temperature, rainfall, and hour attributes. That is, rented_bike_count will be our response variable, and temperature, rainfall, and hour are our predictor attributes. Note that I picked these variables somewhat arbitrarily, so I do not necessarily have a strong intuition for whether these are good predictor attributes for this particular problem. Recall from previous lab activities that we can use the lm function to train a linear model in R. # Train a linear model of rented_bike_count as a function of the temperature, # rainfall, and hour attributes. model_a &lt;- lm( formula = rented_bike_count ~ temperature + rainfall + hour, data = training_data ) summary(model_a) ## ## Call: ## lm(formula = rented_bike_count ~ temperature + rainfall + hour, ## data = training_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1312.14 -292.98 -40.62 228.06 2322.68 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.2286 15.7543 0.586 0.558 ## temperature 27.2239 0.6259 43.495 &lt;2e-16 *** ## rainfall -104.8027 7.0107 -14.949 &lt;2e-16 *** ## hour 31.5550 1.0803 29.210 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 489.9 on 4376 degrees of freedom ## Multiple R-squared: 0.428, Adjusted R-squared: 0.4276 ## F-statistic: 1092 on 3 and 4376 DF, p-value: &lt; 2.2e-16 21.4.1 Computing training and testing error The summary output for our model already gives a lot of useful information about model’s training error (see the Residual standard error). We can also calculate the training error manually: # Use the predict function to get the model&#39;s output for each row of the # training data mA_training_predictions &lt;- predict( model_a, data = training_data ) # Using the training prediction values, we can compute the mean squared error # for the model on the training data. mA_training_MSE &lt;- mean( (training_data$rented_bike_count - mA_training_predictions)^2 ) mA_training_MSE ## [1] 239758.4 On it’s own, the mean squared error (MSE) for a model is a bit hard to interpret. We can also compute the root mean squared error (RMSE): sqrt(mA_training_MSE) ## [1] 489.6513 which gives us a better idea of the average error in the same units as the response variable (i.e., rented_bike_count). Next, let’s compute the testing error of our model. # Use the predict function to get the model&#39;s output for each testing example mA_testing_predictions &lt;- predict( model_a, data = testing_data ) # Compute the mean squared error mA_testing_MSE &lt;- mean( (testing_data$rented_bike_count - mA_testing_predictions)^2 ) mA_testing_MSE ## [1] 591003.2 Once again, we can also compute the root mean squared error (RMSE): sqrt(mA_testing_MSE) ## [1] 768.7673 Notice how our testing error is much worse than our training error. This should make some intuitive sense: the testing data is unseen data not used during training. We do generally expect the testing error to be worse than the training error. 21.5 Comparing two models Next, let’s train a different type of model to predict rented_bike_count as a function of temperature, rainfall, and hour. For this, we’ll train a regression tree (using the rpart) package. # Use rpart to train a regression tree using the training data model_b &lt;- rpart( formula = rented_bike_count ~ temperature + rainfall + hour, data = training_data, parms = list(split=&quot;information&quot;) ) summary(model_b) ## Call: ## rpart(formula = rented_bike_count ~ temperature + rainfall + ## hour, data = training_data, parms = list(split = &quot;information&quot;)) ## n= 4380 ## ## CP nsplit rel error xerror xstd ## 1 0.24260435 0 1.0000000 1.0003448 0.02585794 ## 2 0.16167458 1 0.7573957 0.7649318 0.02080050 ## 3 0.06375902 2 0.5957211 0.6126993 0.01791745 ## 4 0.04179630 3 0.5319621 0.5535851 0.01688811 ## 5 0.02661418 4 0.4901658 0.5047525 0.01635648 ## 6 0.02266353 5 0.4635516 0.4954666 0.01621353 ## 7 0.01917573 6 0.4408880 0.4767022 0.01601860 ## 8 0.01533317 7 0.4217123 0.4434543 0.01541167 ## 9 0.01174074 8 0.4063791 0.4279693 0.01523569 ## 10 0.01116789 9 0.3946384 0.4248556 0.01510883 ## 11 0.01073266 10 0.3834705 0.4118437 0.01503232 ## 12 0.01000000 11 0.3727378 0.4074163 0.01503904 ## ## Variable importance ## hour temperature rainfall ## 44 43 13 ## ## Node number 1: 4380 observations, complexity param=0.2426043 ## mean=706.4888, MSE=419177.7 ## left son=2 (2140 obs) right son=3 (2240 obs) ## Primary splits: ## temperature &lt; 13.05 to the left, improve=0.2426043, (0 missing) ## hour &lt; 6.5 to the left, improve=0.1667917, (0 missing) ## rainfall &lt; 0.05 to the right, improve=0.0464091, (0 missing) ## Surrogate splits: ## hour &lt; 9.5 to the left, agree=0.567, adj=0.114, (0 split) ## ## Node number 2: 2140 observations, complexity param=0.02661418 ## mean=380.2276, MSE=134844.9 ## left son=4 (1054 obs) right son=5 (1086 obs) ## Primary splits: ## temperature &lt; 3.05 to the left, improve=0.1693312, (0 missing) ## hour &lt; 6.5 to the left, improve=0.1511415, (0 missing) ## rainfall &lt; 0.05 to the right, improve=0.0271850, (0 missing) ## Surrogate splits: ## hour &lt; 8.5 to the left, agree=0.525, adj=0.035, (0 split) ## rainfall &lt; 0.15 to the left, agree=0.514, adj=0.012, (0 split) ## ## Node number 3: 2240 observations, complexity param=0.1616746 ## mean=1018.185, MSE=491968.3 ## left son=6 (1436 obs) right son=7 (804 obs) ## Primary splits: ## hour &lt; 15.5 to the left, improve=0.26935740, (0 missing) ## rainfall &lt; 0.05 to the right, improve=0.11946700, (0 missing) ## temperature &lt; 22.15 to the left, improve=0.04916025, (0 missing) ## Surrogate splits: ## temperature &lt; 34.35 to the left, agree=0.645, adj=0.010, (0 split) ## rainfall &lt; 12 to the left, agree=0.642, adj=0.002, (0 split) ## ## Node number 4: 1054 observations ## mean=226.8435, MSE=35717.8 ## ## Node number 5: 1086 observations, complexity param=0.02266353 ## mean=529.0921, MSE=186057.1 ## left son=10 (360 obs) right son=11 (726 obs) ## Primary splits: ## hour &lt; 6.5 to the left, improve=0.20593200, (0 missing) ## rainfall &lt; 0.05 to the right, improve=0.06913844, (0 missing) ## temperature &lt; 8.35 to the left, improve=0.03923234, (0 missing) ## ## Node number 6: 1436 observations, complexity param=0.0417963 ## mean=745.7994, MSE=241882.1 ## left son=12 (531 obs) right son=13 (905 obs) ## Primary splits: ## hour &lt; 6.5 to the left, improve=0.22092860, (0 missing) ## rainfall &lt; 0.15 to the right, improve=0.11635320, (0 missing) ## temperature &lt; 18.85 to the left, improve=0.04268433, (0 missing) ## ## Node number 7: 804 observations, complexity param=0.06375902 ## mean=1504.684, MSE=569442.8 ## left son=14 (68 obs) right son=15 (736 obs) ## Primary splits: ## rainfall &lt; 0.05 to the right, improve=0.25568650, (0 missing) ## temperature &lt; 22.75 to the left, improve=0.06880695, (0 missing) ## hour &lt; 21.5 to the right, improve=0.05142557, (0 missing) ## ## Node number 10: 360 observations ## mean=251.1194, MSE=39242.4 ## ## Node number 11: 726 observations ## mean=666.9298, MSE=201543.4 ## ## Node number 12: 531 observations, complexity param=0.01116789 ## mean=444.0094, MSE=91718.32 ## left son=24 (379 obs) right son=25 (152 obs) ## Primary splits: ## hour &lt; 1.5 to the right, improve=0.42101040, (0 missing) ## rainfall &lt; 0.05 to the right, improve=0.12273650, (0 missing) ## temperature &lt; 18.55 to the left, improve=0.06948794, (0 missing) ## Surrogate splits: ## temperature &lt; 29.55 to the left, agree=0.725, adj=0.039, (0 split) ## ## Node number 13: 905 observations, complexity param=0.01917573 ## mean=922.8718, MSE=245195.9 ## left son=26 (61 obs) right son=27 (844 obs) ## Primary splits: ## rainfall &lt; 0.15 to the right, improve=0.15865810, (0 missing) ## temperature &lt; 16.75 to the left, improve=0.03209958, (0 missing) ## hour &lt; 8.5 to the right, improve=0.02784066, (0 missing) ## ## Node number 14: 68 observations ## mean=249.3382, MSE=214572.3 ## ## Node number 15: 736 observations, complexity param=0.01533317 ## mean=1620.667, MSE=443178.8 ## left son=30 (82 obs) right son=31 (654 obs) ## Primary splits: ## hour &lt; 22.5 to the right, improve=0.08630730, (0 missing) ## temperature &lt; 22.55 to the left, improve=0.05843298, (0 missing) ## ## Node number 24: 379 observations ## mean=319.5646, MSE=42156.49 ## ## Node number 25: 152 observations ## mean=754.3026, MSE=80400.58 ## ## Node number 26: 61 observations ## mean=189.2131, MSE=65229.64 ## ## Node number 27: 844 observations ## mean=975.8969, MSE=216489 ## ## Node number 30: 82 observations ## mean=1068.341, MSE=135121.7 ## ## Node number 31: 654 observations, complexity param=0.01174074 ## mean=1689.919, MSE=438758.4 ## left son=62 (93 obs) right son=63 (561 obs) ## Primary splits: ## hour &lt; 16.5 to the left, improve=0.07512157, (0 missing) ## temperature &lt; 20.55 to the left, improve=0.05371879, (0 missing) ## Surrogate splits: ## temperature &lt; 36.35 to the right, agree=0.861, adj=0.022, (0 split) ## ## Node number 62: 93 observations ## mean=1244.022, MSE=272240.7 ## ## Node number 63: 561 observations, complexity param=0.01073266 ## mean=1763.838, MSE=427938.7 ## left son=126 (244 obs) right son=127 (317 obs) ## Primary splits: ## temperature &lt; 22.35 to the left, improve=0.08207959, (0 missing) ## hour &lt; 19.5 to the right, improve=0.04471757, (0 missing) ## Surrogate splits: ## hour &lt; 21.5 to the right, agree=0.576, adj=0.025, (0 split) ## ## Node number 126: 244 observations ## mean=1550.217, MSE=418431.9 ## ## Node number 127: 317 observations ## mean=1928.265, MSE=373094.9 We can look at the regression tree visually: rpart.plot(model_b) And then we can look more closely at the training error for our regression tree: # Use the predict function to compute the output of our model for each training # example mB_training_predictions &lt;- predict( model_b, data = training_data ) # Compute the mean squared error mB_training_MSE &lt;- mean( (training_data$rented_bike_count - mB_training_predictions)^2 ) mB_training_MSE ## [1] 156243.4 RMSE (for the training data): sqrt(mB_training_MSE) ## [1] 395.2763 If you compare the training error for the regression tree to the training error for our linear model, you’ll see that the regression tree has lower training error. But, training error really isn’t a great way to compare the quality of two models. Remember, we often prefer models that we expect to generalize well to unseen data. Comparing each of the two model’s error on the testing set can give us a better idea of which model might generalize better. To do so, we’ll need to compute the regression tree’s error on the testing set: mB_testing_predictions &lt;- predict( model_b, data = testing_data ) # Mean squared error mB_testing_MSE &lt;- mean( (testing_data$rented_bike_count - mB_testing_predictions)^2 ) mB_testing_MSE ## [1] 669019 RMSE (for the testing data): sqrt(mB_testing_MSE) ## [1] 817.9358 The regression tree’s testing error is much worse than the simple linear model’s testing error, suggesting that the simple linear model might generalize better to unseen data as compared to our regression tree. 21.6 Exercises Identify any lines of code that you do not understand. Use the documentation to figure out what is going on. We did not use all of the attributes in the bike rental data set to build our models. Are there attributes that we didn’t use that you think would be useful to include in a predictive model? Try building a couple of different models using the predictor attributes that you think would be most useful. How do their training/testing errors compare? The modelr package has all sorts of useful functions for assessing model quality. Read over the modelr documentation: https://modelr.tidyverse.org/. Try using some of the functions to compute different types of training/testing errors for the models we trained in this lab activity. Think about the structure of the dataset. What kinds of issues might run into when we randomly divide the data into a training and testing set? For example, are we guaranteed to have a good representation of every hour of the day in both our training and testing data if we split the data randomly? What other sampling procedures could we use to perform a training/testing split that might avoid some of these issues? 21.7 References Dua, D. and Graff, C. (2019). UCI Machine Learning Repository http://archive.ics.uci.edu/ml. Irvine, CA: University of California, School of Information and Computer Science. "],["k-fold-cross-validation-in-r.html", "Chapter 22 K-fold cross validation in R 22.1 Dependencies 22.2 Loading the data 22.3 Cross-validation with crossv_kfold 22.4 Exercises 22.5 References", " Chapter 22 K-fold cross validation in R Cross-validation is another method of estimating the quality of prediction of a model. Often, you’ll want to use cross-validation in situations where your dataset is small, in which case splitting the data into two parts – a training set and a testing set – does not result in a good prediction. That is, your training set might be too small to train a good model, and your testing set might be too small to get a good idea of the quality of your model’s predictions on unseen data. In k-fold cross-validation, you divide the entire data set into k equal-size subsets, and use k-1 parts for training and the remaining part for testing and calculating the prediction error (i.e., prediction quality). You repeat this procedure k times (one for each of the k “folds”), and report the average quality of prediction from the k runs. Fortunately, R has a variety of tools that help you to perform a k-fold cross validation. In this lab activity, we will use functions from the tidyverse collection of packages to perform a k-fold cross validation. 22.1 Dependencies We’ll use the following packages (you will need to install any that you don’t already have installed): library(tidyverse) library(modelr) 22.2 Loading the data In this lab activity, we’ll use the mtcars dataset, which comes built-in with your R install. Read more about it by running ?mtcars in your R console on RStudio. data &lt;- mtcars 22.3 Cross-validation with crossv_kfold We can use the crossv_kfold function in the modelr package to split our data into k exclusive partitions (i.e., k folds). kfolds &lt;- crossv_kfold( data, k=8 ) kfolds ## # A tibble: 8 × 3 ## train test .id ## &lt;named list&gt; &lt;named list&gt; &lt;chr&gt; ## 1 &lt;resample [28 x 11]&gt; &lt;resample [4 x 11]&gt; 1 ## 2 &lt;resample [28 x 11]&gt; &lt;resample [4 x 11]&gt; 2 ## 3 &lt;resample [28 x 11]&gt; &lt;resample [4 x 11]&gt; 3 ## 4 &lt;resample [28 x 11]&gt; &lt;resample [4 x 11]&gt; 4 ## 5 &lt;resample [28 x 11]&gt; &lt;resample [4 x 11]&gt; 5 ## 6 &lt;resample [28 x 11]&gt; &lt;resample [4 x 11]&gt; 6 ## 7 &lt;resample [28 x 11]&gt; &lt;resample [4 x 11]&gt; 7 ## 8 &lt;resample [28 x 11]&gt; &lt;resample [4 x 11]&gt; 8 Notice that the output of crossv_kfold is a tibble (i.e., a fancy data frame) where each row gives a different training/testing split. Each one of those resample objects you see in the tibble can be turned back into a data frame using as.data.frame. For example, # Convert one of the training samples into a more familiar data object as.data.frame(kfolds$train[1]) ## X1.mpg X1.cyl X1.disp X1.hp X1.drat X1.wt X1.qsec X1.vs ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 ## X1.am X1.gear X1.carb ## Mazda RX4 Wag 1 4 4 ## Datsun 710 1 4 1 ## Hornet 4 Drive 0 3 1 ## Hornet Sportabout 0 3 2 ## Duster 360 0 3 4 ## Merc 240D 0 4 2 ## Merc 230 0 4 2 ## Merc 280 0 4 4 ## Merc 280C 0 4 4 ## Merc 450SL 0 3 3 ## Merc 450SLC 0 3 3 ## Cadillac Fleetwood 0 3 4 ## Lincoln Continental 0 3 4 ## Chrysler Imperial 0 3 4 ## Fiat 128 1 4 1 ## Honda Civic 1 4 2 ## Toyota Corolla 1 4 1 ## Toyota Corona 0 3 1 ## Dodge Challenger 0 3 2 ## AMC Javelin 0 3 2 ## Camaro Z28 0 3 4 ## Pontiac Firebird 0 3 2 ## Fiat X1-9 1 4 1 ## Porsche 914-2 1 5 2 ## Ford Pantera L 1 5 4 ## Ferrari Dino 1 5 6 ## Maserati Bora 1 5 8 ## Volvo 142E 1 4 2 We can use the map function (from the purrr tidyverse package) to train a simple linear model (using lm) for each of our 8 folds: # In this code, the map function runs the provided function (lm in this case) # on each of the training sets inside of the kfolds tibble we created. # After running this line, models_a will contain 8 models, each trained on one # of our 8 training sets. models_a &lt;- map(kfolds$train, ~lm(mpg ~ wt, data = .)) Next, we can use the map2_dbl function (from the purrr tidyverse package) to get the testing errors (root mean squared error, rmse in this case) for each model trained on a different fold: # In short, the map2_dbl function &quot;loops&quot; over the contents of the first two # arguments (models_a and kfolds$test), applying each pair of values (one from # models_a and one from kfolds$test) to the function given in the third argument # for the map2_dbl function (rmse). errors_a &lt;- map2_dbl(models_a, kfolds$test, rmse) It’s common to then report the average error across all folds mean(errors_a) ## [1] 2.899075 Cross-validation is particularly useful for comparing across different models. For example, let’s train a slightly more complicated linear model (with wt and hp as predictors): # Again, we use map and map2_dbl to help us train a model and compute its test # error. models_b &lt;- map(kfolds$train, ~lm(mpg ~ wt + hp, data = .)) errors_b &lt;- map2_dbl(models_b, kfolds$test, rmse) mean(errors_b) ## [1] 2.500794 And a regression tree with wt and hp as predictors for mpg: models_c &lt;- map(kfolds$train, ~rpart(mpg ~ wt + hp, data = .)) errors_c &lt;- map2_dbl(models_c, kfolds$test, rmse) mean(errors_c) ## [1] 4.248666 22.4 Exercises The map family of functions from the purrr tidyverse package are pretty powerful. Check your understanding of how they work using the documentation: https://purrr.tidyverse.org/reference/map.html Why might k-fold cross validation be a more robust method of evaluation than performing a single training/testing split (as we have done before) when working with small datasets? 22.5 References Data mining - a knowledge discovery approach (textbook) Cross Validation in R example R for Data Analytics modelr documentation "],["homework-5.html", "Chapter 23 Homework 5 23.1 Overview 23.2 Part A.", " Chapter 23 Homework 5 23.1 Overview In this assignment, you will reflect on one of the readings from Week 8 on either model interpretability or model fairness. This assignment is to be completed individually. 23.1.1 Objectives Gain experience reading and interpretting peer-reviewed academic publications Describe, at a high level, the findings presented in an academic paper Connect a paper’s findings back to your own data mining work 23.1.2 Grading Uploaded requested files, 5% File is properly/clearly formatted, 5% Writing is clear and appropriately formal, 5% Part A Addresses Q1: 5% Addresses Q2: 10% Addresses Q3: 35% Addresses Q4: 35% 23.1.3 Deliverables A .pdf file with your responses 23.1.4 Formatting Include headings for each question with each response underneath the appropriate heading. You do not need to use R markdown to generate your pdf for this assignment. 23.2 Part A. For Part A, choose one of the readings from Week 8 on either model interpretability or model fairness that you found most interesting or useful (one of the articles listed below). Each of these articles is uploaded as a pdf under Week 8’s weekly content. Burrell, Jenna. “How the Machine ‘Thinks’: Understanding Opacity in Machine Learning Algorithms.” Big Data &amp; Society 3, no. 1 (June 1, 2016): 205395171562251. https://doi.org/10.1177/2053951715622512. Poursabzi-Sangdeh, Forough, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Wortman Vaughan, and Hanna Wallach. “Manipulating and Measuring Model Interpretability.” In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, 1–52. Yokohama Japan: ACM, 2021. https://doi.org/10.1145/3411764.3445315. Goodman, Bryce, and Seth Flaxman. “European Union Regulations on Algorithmic Decision-Making and a ‘Right to Explanation.’” AI Magazine 38, no. 3 (October 2, 2017): 50–57. https://doi.org/10.1609/aimag.v38i3.2741. Birhane, Abeba. “Algorithmic Injustice: A Relational Ethics Approach.” Patterns 2, no. 2 (February 2021): 100205. https://doi.org/10.1016/j.patter.2021.100205. Mitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. “Model Cards for Model Reporting.” In Proceedings of the Conference on Fairness, Accountability, and Transparency, 220–29. Atlanta GA USA: ACM, 2019. https://doi.org/10.1145/3287560.3287596. Answer the following questions. Your responses should be in complete sentences. None of your responses need to be more than a paragraph, but it should be clear that you have read and made an effort to understand the chosen paper. Which paper did you select? Why did you choose to read the paper you selected? In your own words (1 - 2 paragraphs), what is the paper about? E.g., what are the goals and/or findings? How could you apply the ideas presented in the article to your own future data mining projects (in this class and/or beyond the classroom)? Provide specific examples. "],["interactive-k-means-clustering-example.html", "Chapter 24 Interactive k-means clustering example", " Chapter 24 Interactive k-means clustering example Below is an interactive k-means clustering demo. You may also access the demo here. In the demo, the black square is your data “canvas”. Click the “drop points” button to populate the canvas with random points. You can also place points on the canvas manually by clicking to drop a point. The clear button will clear all of the currently drawn points. Configure k to set the number of clusters. Press “cluster mode” to cluster. You can run kmeans step-by-step by repeatedly pressing the “step” button. You can tell the demo to just run kmeans continuously with the “run” button. You can still add points to the canvas by clicking in cluster mode. Press “data mode” to go back to configuring. "],["simple-k-means-clustering-in-r.html", "Chapter 25 Simple K-means clustering in R 25.1 Dependencies and setup 25.2 Loading and preprocessing the data 25.3 Running k-means 25.4 What if you don’t know the best number of clusters a priori? 25.5 Exercises 25.6 Other useful R packages when performing K-means clustering 25.7 References", " Chapter 25 Simple K-means clustering in R In this lab activity, we will load a dataset and apply k-means clustering using the built-in kmeans function in R. use within-cluster sum of square error to help us choose a good number of clusters for the k-means algorithms 25.1 Dependencies and setup We’ll use the following packages in this lab activity (you will need to install any that you don’t already have installed): library(tidyverse) library(cowplot) # Used for ggplot plot theme library(khroma) # Used for color scheme for ggplot theme_set(theme_cowplot()) # Set ggplot theme to the cowplot theme 25.2 Loading and preprocessing the data In this lab activity, we’ll be using some toy data in points.csv, which you can find attached to this lab on blackboard or here. # Load data from file into a tibble (a fancy data frame). # You will need to adjust the file path to run this locally. data &lt;- read_csv(&quot;lecture-material/week-09/points.csv&quot;) ## Rows: 400 Columns: 2 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): attr0, attr1 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # data &lt;- read_csv(&quot;points.csv&quot;) This dataset has 400 with two numeric attributes: attr0 and attr1. colnames(data) ## [1] &quot;attr0&quot; &quot;attr1&quot; We can graph the raw data: ggplot( data, aes(x=attr0, y=attr1) ) + geom_point() To perform k-means clustering in R, the data should almost always be prepared as follows: Rows are observations/objects that you want to cluster, columns are attributes Missing values should be removed or estimated Each column (attribute) should be standardized to make attributes comparable. E.g., you might standardize an attribute by tranforming the variables such that they have a mean of 0 and a standard deviation of 1. Fortunately, our data already satisfy the first two requirements. We just need to standardize each column, which we can do with the scale function in R: # Run ?scale to read about how the scale function works in R data$attr0 &lt;- scale(data$attr0) data$attr1 &lt;- scale(data$attr1) Let’s graph our data again now that we’ve scaled each column. ggplot( data, aes(x=attr0, y=attr1) ) + geom_point() 25.3 Running k-means We can use the built-in kmeans function to perform k-means clustering in R. kmeans only works with numeric columns. If you want to use cluster data based on categorical/non-numeric columns, you’ll need to either convert those columns to numbers such that distances between the numeric values you convert to correspond with the distances between categories/symbols. Otherwise, you might need to look for other implementations of k-means that allows for more customization, or you might need to implement k-means yourself with your own custom distance measure. Note that kmeans does not let you configure the distance function it uses to perform clustering. Read more about common distance functions here: https://uc-r.github.io/kmeans_clustering#distance Unfortunately, if you want to customize the distance function, you’ll need to find a more customizable implementation of k-means clustering or implement it yourself. Fortunately, the k-means algorithm is fairly straightforward, making it not too bad to write your own implementation: https://uc-r.github.io/kmeans_clustering#kmeans We know from looking at our raw data that it probably makes sense to cluster our data into 4 clusters: # Run ?kmeans to read about each of the function arguments used below cluster_results &lt;- kmeans( data, centers = 4, # K, number of clusters to partition data into iter.max = 100, nstart = 10, # Try 10 different starting centroids, pick best one algorithm = &quot;Lloyd&quot; # Particular version of kmeans algorithm ) cluster_results ## K-means clustering with 4 clusters of sizes 100, 100, 100, 100 ## ## Cluster means: ## attr0 attr1 ## 1 0.9196341 0.9803683 ## 2 0.9169208 -0.9296131 ## 3 -0.8919481 0.8841333 ## 4 -0.9446068 -0.9348886 ## ## Clustering vector: ## [1] 2 1 2 2 2 2 4 4 4 2 4 4 4 4 4 3 1 4 4 4 2 4 4 1 1 1 1 3 4 4 3 4 3 2 2 2 1 ## [38] 3 1 1 3 4 2 2 4 2 2 2 3 3 3 2 2 1 3 2 4 1 2 1 1 4 4 2 3 2 4 3 3 1 1 2 4 1 ## [75] 2 2 3 2 2 1 1 1 2 1 4 2 3 4 1 3 2 3 2 4 2 2 1 3 4 1 2 3 4 3 2 3 1 1 4 4 2 ## [112] 3 3 2 3 3 2 3 1 1 3 4 2 2 3 2 1 2 3 2 1 4 4 3 3 4 2 2 3 1 3 4 1 3 1 4 1 1 ## [149] 3 3 3 2 1 4 1 3 4 4 3 1 4 4 3 1 4 3 1 2 4 4 4 4 1 2 1 3 4 4 2 4 1 2 3 3 1 ## [186] 1 3 1 2 1 3 3 4 1 4 4 1 4 3 3 1 1 3 4 1 3 1 3 3 4 3 3 3 1 3 4 3 2 2 2 1 1 ## [223] 4 3 4 1 1 4 4 3 1 1 1 2 2 3 4 1 4 3 4 2 3 4 3 1 1 2 2 2 3 3 1 2 4 3 4 3 3 ## [260] 4 2 3 1 4 1 1 2 3 2 2 3 2 2 2 4 3 2 2 3 1 2 1 3 3 3 3 1 4 1 3 2 4 3 2 1 4 ## [297] 3 4 2 3 3 4 2 1 3 3 2 3 2 1 4 4 1 1 2 1 4 2 2 1 4 4 3 4 2 4 1 3 4 1 4 4 1 ## [334] 2 4 4 4 3 1 2 4 1 3 3 1 2 3 2 2 2 1 3 4 3 1 1 3 4 1 2 2 2 1 4 1 1 3 2 1 1 ## [371] 2 2 2 1 1 4 4 2 2 4 1 2 1 2 1 4 4 4 3 4 2 2 4 3 4 3 3 2 1 1 ## ## Within cluster sum of squares by cluster: ## [1] 32.05632 25.06436 27.71012 27.63571 ## (between_SS / total_SS = 85.9 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; cluster_results$cluster has the cluster ids for each row in our dataset. To make graphing our clustered data easier, let’s add a cluster_id column to our dataset. # Add column to data that specifies the cluster that each row belongs to data$cluster_id &lt;- as.factor(cluster_results$cluster) head(data) ## # A tibble: 6 × 3 ## attr0[,1] attr1[,1] cluster_id ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.821 -1.45 2 ## 2 0.337 1.08 1 ## 3 1.27 -0.625 2 ## 4 1.42 -1.21 2 ## 5 0.935 -0.297 2 ## 6 0.486 -1.56 2 Next, let’s graph our data but we’ll color points based on their cluster membership. Additionally, we’ll overlay the centroids (stored in cluster_results$centers) as black squares on our data. # Create a dataframe with cluster centers that we can plot cluster_centers &lt;- as.data.frame(cluster_results$centers) # Convert cluster_id column into a factor # cluster_centers$cluster_id &lt;- as.factor(cluster_centers$cluster_id) head(cluster_centers) ## attr0 attr1 ## 1 0.9196341 0.9803683 ## 2 0.9169208 -0.9296131 ## 3 -0.8919481 0.8841333 ## 4 -0.9446068 -0.9348886 ggplot( data, aes( x = attr0, y = attr1, color = cluster_id, fill = cluster_id ) ) + geom_point() + geom_point( data=cluster_centers, shape=15, fill=&quot;black&quot;, color=&quot;black&quot;, size=5 ) + scale_color_bright( name = &quot;Cluster ID&quot; ) + scale_fill_bright( name = &quot;Cluster ID&quot; ) You are not guaranteed to get the same clusters everytime you run k-means. The clusters you end up can change depending on the initial centroids chosen. 25.4 What if you don’t know the best number of clusters a priori? The k-means algorithm requires that you set the number of clusters you want to partition your data into as a parameter. In the example above, we looked at the data (which was conveniently 2-dimensional), and we decided that we should have 4 clusters. But often, your data are many-dimensional, and it is obvious how many clusters you use a priori. In these situations, we can run k-means with different values for k and choose the k that gives us the “best clustering”. But how do we assess the quality of a clustering? There are three common methods for choosing the best number of clusters (i.e., the best value for k): Elbow method Silhouette method Gap statistic Read more about each of those methods here: https://uc-r.github.io/kmeans_clustering#optimal. Here, we’ll demonstrate the simplist of those three: the Elbow method. For the elbow method, we assess our clustering results for a particular k by measuring the within-cluster variation (i.e., total within-cluster sum of square error). The total within-cluster sum of square error measures the “compactness” of a cluster. In the extreme, you can minimize total within-cluster sum of square error (i.e., maximize compactness) by having each individual data point in its own cluster, which results in 0 within-cluster variation. However, that’s not particularly useful from a data mining perspective. In general, you want the smallest value of k (i.e., the fewest number of clusters) where each cluster is still reasonably “compact”. If that all sounds a little vague to you, it is! With the elbow method, you plot the total within-cluster sum of square error for different values of k, and choose the k where the curve “bends” and you start getting diminishing returns in compactness for increasing k. # wss will hold the within-cluster square error for running kmeans on our data # for k values ranging from 2 to 10. That is, we run kmeans 9 times, once per k. wss &lt;- data.frame(k=integer(),wss=double()) # k_range gives the range of k values we&#39;re going to try k_range &lt;- 2:10 # For each k value we want to try, run kmeans and record the within-cluster square # error. for (k in k_range) { error &lt;- kmeans(data, k, nstart=10)$tot.withinss wss &lt;- rbind(wss, list(&quot;k&quot;=k, &quot;wss&quot;=error)) } head(wss) ## k wss ## 1 2 560.44901 ## 2 3 328.04719 ## 3 4 112.46651 ## 4 5 98.45568 ## 5 6 86.32249 ## 6 7 75.24017 Next, we plot the within-cluster squared error that we measured for each value of k that we tried. ggplot( data = wss, mapping = aes(x=k,y=wss) ) + geom_point() + geom_line() + scale_y_continuous( name=&quot;Within-cluster sum of square error (wss)&quot; ) + scale_x_continuous( name=&quot;k (number of clusters)&quot;, breaks=k_range ) In this case, the “elbow” of the curve is pretty obvious at 4. In real-world data, it might be similarly obvious. Or, it’ll be a little less obvious, and you might run your entire data mining pipeline for a couple of decent values of k. I strongly encourage you to read up on the other methods of choosing a good k: https://uc-r.github.io/kmeans_clustering#optimal. 25.5 Exercises Identify any lines of code that you do not understand. Use the documentation to figure out what is going on. Plot cluster assignment for running kmeans with k=2 and k=8. Compare those assignments with k=4. Which had the highest within-cluster square error? Which had the lowest? Discuss your findings. Discuss what you would need to do to apply k-means clustering to non-numeric data. Describe a step-by-step process by which you could use k-means clustering to cluster Tweets. 25.6 Other useful R packages when performing K-means clustering cluster factoextra 25.7 References https://uc-r.github.io/kmeans_clustering – If you want to go a step deeper into using k-means clustering, I strongly encourage you to work through this article (which includes R code) "],["final-project-overview.html", "Chapter 26 Final project overview 26.1 Problem statement 26.2 Learning objectives 26.3 Groups 26.4 Grading 26.5 Timeline 26.6 Possible project ideas", " Chapter 26 Final project overview In this data mining project, you will choose a data mining application domain and develop a data mining pipeline for a specific problem. 26.1 Problem statement One of the goals for this course is for you to be able to design, implement, and evaluate a data mining / knowledge discovery pipeline from start to finish. You will propose and implement a data mining project in an application domain of your choice. You will be guided on your project through a series of check-in assignments (listed below), culminating in a written final report. Project survey (see blackboard, under Assignments) Project proposal Project progress report Project report draft Project final report 26.2 Learning objectives Design and implement a full data mining pipeline Develop skills for planning a data mining project and conducting background research Gain proficiency with data mining algorithms and techniques Effectively communicate your methodology and results 26.3 Groups You will be allowed to work in groups of three or work by yourself. I will assist in forming groups by sending out a survey asking about which topics interest you, who you would/would not like to work with (if anyone), and your expectations for your final project. 26.4 Grading 40% of your grade in this course relates to your final project: Item Contribution to grade Survey response 2.5% Project proposal 10% Project update 2.5% Draft 5% Final report 20% Note that you cannot use your homework extensions on project components. All components of your final project are subject to GVSU and the School of Computing’s academic honesty policies. Violations of these policies may result in failure from the course. 26.5 Timeline Week Project Component Week 5 Survey out (Monday, 09/26) Week 6 Survey due (Monday, 10/03) Week 7 Groups assigned (by Monday, 10/10) Week 8 Week 9 Project proposal due (Wednesday, 10/26) Week 10 Receive proposal decision and feedback (by Monday, 10/31) Week 11 Week 12 Progress report due (Wednesday, 11/16) Week 13 Week 14 Project report draft due (Wednesday, 11/30) Week 15 Receive feedback on draft Week 16 Final project due (Wednesday, 12/14) 26.6 Possible project ideas I do understand that we have not covered many data mining techniques at this point in the class. However, we have overviewed broad categories of data mining tasks. My advice is to start by choosing a domain that is interesting to you and a type of data mining task that you think is applicable (and that you are interested in learning more about). You will then need to do your own research on the specifics. Some example types of projects (your project does not need to fall under any of these categories) Solve a problem E.g., Design and apply a data mining pipeline to a particular problem you are interested in, and then evaluate your approach by comparing to another method. Replicate (and/or extend) a published study E.g., identify a published study that you are interested in and independently replicate some portion of it (you must create your own pipeline to do so, not just reuse the original authors’). Answer a research question E.g., formulate a hypothesis and design (and apply) a data mining approach to test your hypothesis. Analysis E.g., design a data mining pipeline for some task (or use an existing pipeline) and analyze how/why it works (or doesn’t work). You might try swapping out individual components of the pipeline for alternative techniques, add noise to the dataset, etc, etc. E.g., design an experiment to analyze the properties of different similarity metrics and their effect on a data mining algorithm You are welcome to drop by office hours to chat about project ideas! "],["final-project---proposal.html", "Chapter 27 Final project - Proposal 27.1 Project expectations 27.2 Proposal guidelines 27.3 Grading", " Chapter 27 Final project - Proposal Your project proposal serves two purposes: for you to end up with a solid plan for your project, and for me to evaluate whether your project fits my expectations and help you to improve in where it doesn’t 27.1 Project expectations In your project, you will choose a data mining application domain and develop a data mining pipeline for a specific problem. The particular application area, techniques you choose to use, and the outputs of your project are open-ended and up to you to propose. Be creative! However, I do have the following expectations for all projects: I expect you to design and implement a complete data mining pipeline from start to finish. I expect your project to be well-researched, appropriate, and achievable in the given time frame. If you’re unsure, come chat with me during office hours! You may use existing software/libraries to implement your pipeline. However, you must implement at least one component (i.e., data collection, data mining algorithm) of your project yourself. For example, if your project involves extensive data preprocessing and then the use of symbolic regression to build a model that you plan to compare to an alternative approach, you may choose to implement your symbolic regression algorithm and use existing libraries for your data preprocessing and for the alternative model. I expect you to either collect your own dataset or use a publicly available dataset. I expect you to evaluate your data mining approach in some way. For example, conducting a small study that compares your approach to an alternative approach, assessing the accuracy of your model by holding back validation data, accessing the robustness of your model, etc. I expect you to effectively communicate your methodology and findings in a final report. This will take the form of a well-organized and well-written explanations with publication-quality visualzations. I will consider the overall difficulty of your project when determining whether or not it is sufficient (or infeasible) for your final project. For example, if your are collecting your own data, your pipeline will need to be simpler because of the extra time and effort requried for data collection. 27.2 Proposal guidelines Your project proposal should be under 3 pages, be formatted under the following headings, and address each of the following points: Project overview Provide an overview of your project. What is the application domain you are working on? What specific problem are you working on? Briefly, what is your overall approach? Why? What is your motivation? What are your goals for this project? Related work What similar projects have been attempted in your proposed domain? How do they related to your project? Data plan What data do you plan to use for your project? If you plan on collecting your own data, how do you plan to do so? If you plan to use an existing dataset, which dataset do you plan to use and why? Who collected your data and for what original purpose? What proprocessing do you expect to need to perform on your data? Implementation plan Describe your proposed data mining pipeline How will you implement your data mining pipeline? What major component do you plan to implement yourself? What components do you plan to use existing software/libraries for? Which software/libraries do you plan to use? Evaluation plan How do you plan to evaluate your data mining algorithm? Provide a broad overview of how you plan to evaluate your approach. How will you measure success? Will you compare to an alternative method? Measure accuracy with a ground truth testing set? Plan for group collaboration What is your plan for collaborating as a group? Do you plan to meet regularly in-person, over zoom, or coordinate asynchronously on a messaging application like discord? How do you plan to collaboratively implement your data mining pipeline? How will you manage your code and your data? Timeline Outline your week-by-week goals for completing your project by 11/30 Feel free to communicate this timeline as a bulleted list, a table, etc. References List your references. Include a project title and the members of your group as part of your proposal. Your proposal should be organized into clearly labeled sections (one to address each of the eight categories above). Each section should be written in paragraph form (with the exception of the references and the timeline section where you may opt to use a list or a table). Your proposal will be graded on the quality of your writing as well as your answers to the previously listed prompts. It should be evident that your group has thoroughly researched the area and formulated a plan to achieve your goals. I recognize that things may change during the course of the project, but I’m looking here to make sure you have a solid initial plan. Any significant project like this requires significant prework before implementation. 27.3 Grading The proposal will be graded as follows (out of 100): Item Points Project overview section addresses required points 10 Related work section addresses required points 15 Data plan section addresses required points 10 Implementation plan section addresses required points 15 Evaluation plan section addresses required points 10 Plan for group collaboration section addresses required points 10 Timeline section addresses required points 10 Related work and references are included and cited appropriately 5 Writing is appropriately formal 5 Writing is clear 5 Proposal is well-formatted 5 I reserve the right to award flex points for exemplary work in any areas of this project. All components of your final project are subject to GVSU and the School of Computing’s academic honesty policies. Violations of these policies may result in failure from the course. "],["final-project---mid-project-update.html", "Chapter 28 Final project - Mid-project update 28.1 Requirements 28.2 Grading", " Chapter 28 Final project - Mid-project update The goal of this progress report is for you to have space to reflect on your progress so far and plan. You should be working on your final project consistently instead of leaving everything to the end of the semester. The progress report is a mechanism to make that more likely. You should not wait for my feedback on your progress update before resuming work on your project 🙂 28.1 Requirements Your progress update should be approximately one to two pages of text and address the following: Progress so far What have you completed so far? Provide a link to your data so far Provide a link to your code so far (e.g., github repository, etc) Challenges Have you run into any challenges so far? Has anything taken longer than expected? How have you overcome these challenges? If there are any current challenges you have not yet overcome, how do you plan to overcome them? Collaboration (if applicable) How frequently does your group meet? Does everyone feel like each group member is contributing meaningfully to the project? If not, how does your group plan address this problem? Next steps What do you still need to complete for your project? What is your plan to complete your project? Do you forsee any potential challenges? Organize your progress report into three sections (one for each of the categories above). 28.2 Grading Graded on the following out of 100: Item Points Addressed the required points 80 Document is clearly organized 10 Writing is sufficiently formal and clear 10 All components of your final project are subject to GVSU and the School of Computing’s academic honesty policies. Violations of these policies may result in failure from the course. "],["final-project---report-draft.html", "Chapter 29 Final project - Report draft 29.1 Grading", " Chapter 29 Final project - Report draft This draft of your report is your opportunity to get my feedback on your report. Your draft does not need to be complete. I will give you feedback on it regardless of the state you turn it in (though, it does need to be well organized with proper section headings to facilitate me being able to give feedback). The more complete of a report you turn in, the more detailed feedback I can give you for what you need to do in order to earn the grade you want on your final report. For example, if you turn in a complete draft of your final report, I can tell you what grade you would receive if you turned it in as your final report and tell you where improvements can/need to be made. If your project is not complete by the time you turn in your draft, you should not wait for my feedback on your draft before resuming work on your project 🙂 29.1 Grading Your draft will be graded primarily on Effort: turned it in on time, draft contains meaningful content relevant to your final report, and it is clear that you have completed a meaningful amount of your project I can’t give meaningful feedback if you don’t submit a meaningful draft. I cannot give full credit for effort if you do not demonstrate that you have at least completed some meaningful portion of your project. Organization: clearly organized with proper section headings, text is readable I can’t give you useful feedback if your draft is not organized in a reasonable way. Graded on the following out of 100: Item Points Effort 80 Document is clearly organized 20 "],["final-project---final-report.html", "Chapter 30 Final project - Final report 30.1 Format 30.2 Contents 30.3 Deliverables 30.4 Grading", " Chapter 30 Final project - Final report Documenting your work is as important as building your data mining pipeline and generating results. If you don’t document your pipeline and your results and present them in an easily understood format, no one will ever know what you found! 30.1 Format Your final report can be prepared in a format of your choice. However, it must be clearly organized, use consistent formatting for citations/references, contain the required contents (see below), and I must be able to access it. To be safe, I would advise you to check with me before moving forward with a format. I cannot grade your report if I cannot access it. It is your responsibility to make sure that I can access it before you submit. (don’t rely on me to answer my email after 5:00pm on the day your assignment is due 😉) Example formats you might choose to compose your report in: Using R markdown to create a web-accessible HTML page. This could support interactivity and allow you to embed code into your report. Using R markdown and Bookdown to create an eBook. This could support interactivity and allow you to embed code into your report. Using LaTex to create a PDF If you use LaTex, I suggest using Overleaf–it’s like google docs, but for LaTex. Using a conventional text editor to create a PDF If you plan to use this project in a larger portfolio to send to potential employers, I strongly recommend making your report into a well-formatted (and interactive if possible) web page. I am happy to explain how to do so if you are interested. 30.2 Contents Your report should include the following sections: Introduction Related work Methods Results and Discussion Conclusion Data and Software Availability References You can (and probably should) include subsections within those sections. You may also embed code and its output directly into your document (e.g., as you would if using R markdown). If you want to deviate from this structure for some reason, you need to speak with me ahead of time (at least a week before the deadline). Good reasons for deviating from the required sections would be if you plan to submit your work to a conference or competition that requires a different organization, you are creating an interactive document that lends itself to a different organization. Below, I outline the points you must address in each section: 30.2.1 Introduction Your introduction should: introduce your application domain and the specific problem you’re working on, providing enough background for readers to understand the value of your project motivate your project overview your approach overview your results 30.2.2 Related work Your related work section should: discuss previous work done that relates to your project. For example, alternative approaches to the problem you’re trying to solve other examples of your particular approach in your domain or in another domain Your related work should include appropriate references to academic publications (e.g., found using google scholar, pubmed, etc.) A good related work section will likely reference (and overview) several related academic publications. 30.2.3 Methods Your methods section should: describe how you chose your data and/or how you collected your data describe your data mining pipeline in sufficient detail such that someone could replicate your project What data preprocessing did you perform? Which data mining algorithms did you apply (and how)? What postprocessing did you do? describe any analyses you performed to evaluate your model describe the software You may wish to include diagrams that overview methodology. 30.2.4 Results and discussion You results and discussion section should describe your results and discuss how your results fit into a broader context. You should include publication-quality visualizations that effectively communicate your results. Depending on the format you use for your final report, you may also wish to embed interactive visualizations. For example, you might present the performance of your data mining approach and discuss how it compares to a baseline/alternative method. 30.2.5 Conclusion Your conclusion should: very briefly summarize your findings discuss any limitations/shortcomings of your approach/experiments suggest future work (e.g., possible extensions to your project) 30.2.6 Data and software availability Your data and software availability section should: provide a working link to where your software can be found and downloaded (e.g., a github repository) provide a working link to where your data can be downloaded If either of those are not feasible for your project (e.g., you collected your own dataset and it’s huge), let me know at least a week before the deadline (the sooner you let me know, the better). 30.2.7 References Your references should be consistenly formatted such that a reader can find each reference (e.g., include authors, year, title, venue/journal, DOI). If you use Rmd, Bookdown, or LaTex to create your report, I can help you configure things such that your references are automatically formatted for you. 30.3 Deliverables Submit your report through Blackboard. If your report is a .pdf file, attach it to the submission. If your report is a live webpage, include a (working) link. One submission per group. Include in the comments section of the assignment the names of the group members. As part of your report, you will need to include working links to your code and data (e.g., in a github repository). 30.4 Grading Your final report will be graded on the following out of 100: Item Points Title accurately describes your project/findings 5 Introduction 10 Related work 10 Methods 10 Results and discussion 10 Conclusion 10 Data and software availability 10 References are consistently formatted 5 Code can be compiled/executed using provided instructions 10 Report is well-organized into proper sections, paragraphs follow logical structure 10 Report is sufficiently professional (few spelling and gramatical errors) 10 I reserve the right to award flex points for exemplary work in any areas of this project. All components of your final project are subject to GVSU and the School of Computing’s academic honesty policies. Violations of these policies may result in failure from the course. "],["references-4.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
